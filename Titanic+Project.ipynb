{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/isaac/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "/home/isaac/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(123)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#####################################\n",
    "# Import the train and test datasets#\n",
    "#####################################\n",
    "\n",
    "train_data_full = pd.read_csv(\"/home/isaac/Fundamentals_of_Data_Science_Certificate/Extra Python work/Titanic Project/train.csv\")\n",
    "\n",
    "test_data = pd.read_csv(\"/home/isaac/Fundamentals_of_Data_Science_Certificate/Extra Python work/Titanic Project/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Moran, Mr. James</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>330877</td>\n",
       "      <td>8.4583</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>McCarthy, Mr. Timothy J</td>\n",
       "      <td>male</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17463</td>\n",
       "      <td>51.8625</td>\n",
       "      <td>E46</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Palsson, Master. Gosta Leonard</td>\n",
       "      <td>male</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>349909</td>\n",
       "      <td>21.0750</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)</td>\n",
       "      <td>female</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>347742</td>\n",
       "      <td>11.1333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Nasser, Mrs. Nicholas (Adele Achem)</td>\n",
       "      <td>female</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>237736</td>\n",
       "      <td>30.0708</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Sandstrom, Miss. Marguerite Rut</td>\n",
       "      <td>female</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>PP 9549</td>\n",
       "      <td>16.7000</td>\n",
       "      <td>G6</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Bonnell, Miss. Elizabeth</td>\n",
       "      <td>female</td>\n",
       "      <td>58.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>113783</td>\n",
       "      <td>26.5500</td>\n",
       "      <td>C103</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Saundercock, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5. 2151</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Andersson, Mr. Anders Johan</td>\n",
       "      <td>male</td>\n",
       "      <td>39.0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>347082</td>\n",
       "      <td>31.2750</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Vestrom, Miss. Hulda Amanda Adolfina</td>\n",
       "      <td>female</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>350406</td>\n",
       "      <td>7.8542</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Hewlett, Mrs. (Mary D Kingcome)</td>\n",
       "      <td>female</td>\n",
       "      <td>55.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>248706</td>\n",
       "      <td>16.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Rice, Master. Eugene</td>\n",
       "      <td>male</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>382652</td>\n",
       "      <td>29.1250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Williams, Mr. Charles Eugene</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>244373</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Vander Planke, Mrs. Julius (Emelia Maria Vande...</td>\n",
       "      <td>female</td>\n",
       "      <td>31.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>345763</td>\n",
       "      <td>18.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Masselmani, Mrs. Fatima</td>\n",
       "      <td>female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2649</td>\n",
       "      <td>7.2250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Fynney, Mr. Joseph J</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>239865</td>\n",
       "      <td>26.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Beesley, Mr. Lawrence</td>\n",
       "      <td>male</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>248698</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>D56</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>McGowan, Miss. Anna \"Annie\"</td>\n",
       "      <td>female</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>330923</td>\n",
       "      <td>8.0292</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Sloper, Mr. William Thompson</td>\n",
       "      <td>male</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>113788</td>\n",
       "      <td>35.5000</td>\n",
       "      <td>A6</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Palsson, Miss. Torborg Danira</td>\n",
       "      <td>female</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>349909</td>\n",
       "      <td>21.0750</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Asplund, Mrs. Carl Oscar (Selma Augusta Emilia...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>347077</td>\n",
       "      <td>31.3875</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Emir, Mr. Farred Chehab</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2631</td>\n",
       "      <td>7.2250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Fortune, Mr. Charles Alexander</td>\n",
       "      <td>male</td>\n",
       "      <td>19.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>19950</td>\n",
       "      <td>263.0000</td>\n",
       "      <td>C23 C25 C27</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>O'Dwyer, Miss. Ellen \"Nellie\"</td>\n",
       "      <td>female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>330959</td>\n",
       "      <td>7.8792</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Todoroff, Mr. Lalio</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>349216</td>\n",
       "      <td>7.8958</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>861</th>\n",
       "      <td>862</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Giles, Mr. Frederick Edward</td>\n",
       "      <td>male</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>28134</td>\n",
       "      <td>11.5000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>862</th>\n",
       "      <td>863</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Swift, Mrs. Frederick Joel (Margaret Welles Ba...</td>\n",
       "      <td>female</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17466</td>\n",
       "      <td>25.9292</td>\n",
       "      <td>D17</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>863</th>\n",
       "      <td>864</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Sage, Miss. Dorothy Edith \"Dolly\"</td>\n",
       "      <td>female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>CA. 2343</td>\n",
       "      <td>69.5500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>864</th>\n",
       "      <td>865</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Gill, Mr. John William</td>\n",
       "      <td>male</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>233866</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>865</th>\n",
       "      <td>866</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Bystrom, Mrs. (Karolina)</td>\n",
       "      <td>female</td>\n",
       "      <td>42.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>236852</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>866</th>\n",
       "      <td>867</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Duran y More, Miss. Asuncion</td>\n",
       "      <td>female</td>\n",
       "      <td>27.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>SC/PARIS 2149</td>\n",
       "      <td>13.8583</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>867</th>\n",
       "      <td>868</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Roebling, Mr. Washington Augustus II</td>\n",
       "      <td>male</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17590</td>\n",
       "      <td>50.4958</td>\n",
       "      <td>A24</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>868</th>\n",
       "      <td>869</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>van Melkebeke, Mr. Philemon</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>345777</td>\n",
       "      <td>9.5000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>869</th>\n",
       "      <td>870</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Johnson, Master. Harold Theodor</td>\n",
       "      <td>male</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>347742</td>\n",
       "      <td>11.1333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>870</th>\n",
       "      <td>871</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Balkic, Mr. Cerin</td>\n",
       "      <td>male</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>349248</td>\n",
       "      <td>7.8958</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>871</th>\n",
       "      <td>872</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Beckwith, Mrs. Richard Leonard (Sallie Monypeny)</td>\n",
       "      <td>female</td>\n",
       "      <td>47.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>11751</td>\n",
       "      <td>52.5542</td>\n",
       "      <td>D35</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>872</th>\n",
       "      <td>873</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Carlsson, Mr. Frans Olof</td>\n",
       "      <td>male</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>695</td>\n",
       "      <td>5.0000</td>\n",
       "      <td>B51 B53 B55</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>873</th>\n",
       "      <td>874</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Vander Cruyssen, Mr. Victor</td>\n",
       "      <td>male</td>\n",
       "      <td>47.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>345765</td>\n",
       "      <td>9.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>874</th>\n",
       "      <td>875</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Abelson, Mrs. Samuel (Hannah Wizosky)</td>\n",
       "      <td>female</td>\n",
       "      <td>28.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>P/PP 3381</td>\n",
       "      <td>24.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>875</th>\n",
       "      <td>876</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Najib, Miss. Adele Kiamie \"Jane\"</td>\n",
       "      <td>female</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2667</td>\n",
       "      <td>7.2250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>876</th>\n",
       "      <td>877</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Gustafsson, Mr. Alfred Ossian</td>\n",
       "      <td>male</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7534</td>\n",
       "      <td>9.8458</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>877</th>\n",
       "      <td>878</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Petroff, Mr. Nedelio</td>\n",
       "      <td>male</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>349212</td>\n",
       "      <td>7.8958</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>878</th>\n",
       "      <td>879</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Laleff, Mr. Kristo</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>349217</td>\n",
       "      <td>7.8958</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>879</th>\n",
       "      <td>880</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Potter, Mrs. Thomas Jr (Lily Alexenia Wilson)</td>\n",
       "      <td>female</td>\n",
       "      <td>56.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>11767</td>\n",
       "      <td>83.1583</td>\n",
       "      <td>C50</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>880</th>\n",
       "      <td>881</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Shelley, Mrs. William (Imanita Parrish Hall)</td>\n",
       "      <td>female</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>230433</td>\n",
       "      <td>26.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>881</th>\n",
       "      <td>882</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Markun, Mr. Johann</td>\n",
       "      <td>male</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>349257</td>\n",
       "      <td>7.8958</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>882</th>\n",
       "      <td>883</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Dahlberg, Miss. Gerda Ulrika</td>\n",
       "      <td>female</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7552</td>\n",
       "      <td>10.5167</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>883</th>\n",
       "      <td>884</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Banfield, Mr. Frederick James</td>\n",
       "      <td>male</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>C.A./SOTON 34068</td>\n",
       "      <td>10.5000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>884</th>\n",
       "      <td>885</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Sutehall, Mr. Henry Jr</td>\n",
       "      <td>male</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>SOTON/OQ 392076</td>\n",
       "      <td>7.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>885</th>\n",
       "      <td>886</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Rice, Mrs. William (Margaret Norton)</td>\n",
       "      <td>female</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>382652</td>\n",
       "      <td>29.1250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>887</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Montvila, Rev. Juozas</td>\n",
       "      <td>male</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>211536</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>888</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Graham, Miss. Margaret Edith</td>\n",
       "      <td>female</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>112053</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>B42</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>889</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Johnston, Miss. Catherine Helen \"Carrie\"</td>\n",
       "      <td>female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>W./C. 6607</td>\n",
       "      <td>23.4500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>890</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Behr, Mr. Karl Howell</td>\n",
       "      <td>male</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>111369</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>C148</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>891</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Dooley, Mr. Patrick</td>\n",
       "      <td>male</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>370376</td>\n",
       "      <td>7.7500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>891 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     PassengerId  Survived  Pclass  \\\n",
       "0              1         0       3   \n",
       "1              2         1       1   \n",
       "2              3         1       3   \n",
       "3              4         1       1   \n",
       "4              5         0       3   \n",
       "5              6         0       3   \n",
       "6              7         0       1   \n",
       "7              8         0       3   \n",
       "8              9         1       3   \n",
       "9             10         1       2   \n",
       "10            11         1       3   \n",
       "11            12         1       1   \n",
       "12            13         0       3   \n",
       "13            14         0       3   \n",
       "14            15         0       3   \n",
       "15            16         1       2   \n",
       "16            17         0       3   \n",
       "17            18         1       2   \n",
       "18            19         0       3   \n",
       "19            20         1       3   \n",
       "20            21         0       2   \n",
       "21            22         1       2   \n",
       "22            23         1       3   \n",
       "23            24         1       1   \n",
       "24            25         0       3   \n",
       "25            26         1       3   \n",
       "26            27         0       3   \n",
       "27            28         0       1   \n",
       "28            29         1       3   \n",
       "29            30         0       3   \n",
       "..           ...       ...     ...   \n",
       "861          862         0       2   \n",
       "862          863         1       1   \n",
       "863          864         0       3   \n",
       "864          865         0       2   \n",
       "865          866         1       2   \n",
       "866          867         1       2   \n",
       "867          868         0       1   \n",
       "868          869         0       3   \n",
       "869          870         1       3   \n",
       "870          871         0       3   \n",
       "871          872         1       1   \n",
       "872          873         0       1   \n",
       "873          874         0       3   \n",
       "874          875         1       2   \n",
       "875          876         1       3   \n",
       "876          877         0       3   \n",
       "877          878         0       3   \n",
       "878          879         0       3   \n",
       "879          880         1       1   \n",
       "880          881         1       2   \n",
       "881          882         0       3   \n",
       "882          883         0       3   \n",
       "883          884         0       2   \n",
       "884          885         0       3   \n",
       "885          886         0       3   \n",
       "886          887         0       2   \n",
       "887          888         1       1   \n",
       "888          889         0       3   \n",
       "889          890         1       1   \n",
       "890          891         0       3   \n",
       "\n",
       "                                                  Name     Sex   Age  SibSp  \\\n",
       "0                              Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1    Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                               Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3         Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                             Allen, Mr. William Henry    male  35.0      0   \n",
       "5                                     Moran, Mr. James    male   NaN      0   \n",
       "6                              McCarthy, Mr. Timothy J    male  54.0      0   \n",
       "7                       Palsson, Master. Gosta Leonard    male   2.0      3   \n",
       "8    Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)  female  27.0      0   \n",
       "9                  Nasser, Mrs. Nicholas (Adele Achem)  female  14.0      1   \n",
       "10                     Sandstrom, Miss. Marguerite Rut  female   4.0      1   \n",
       "11                            Bonnell, Miss. Elizabeth  female  58.0      0   \n",
       "12                      Saundercock, Mr. William Henry    male  20.0      0   \n",
       "13                         Andersson, Mr. Anders Johan    male  39.0      1   \n",
       "14                Vestrom, Miss. Hulda Amanda Adolfina  female  14.0      0   \n",
       "15                    Hewlett, Mrs. (Mary D Kingcome)   female  55.0      0   \n",
       "16                                Rice, Master. Eugene    male   2.0      4   \n",
       "17                        Williams, Mr. Charles Eugene    male   NaN      0   \n",
       "18   Vander Planke, Mrs. Julius (Emelia Maria Vande...  female  31.0      1   \n",
       "19                             Masselmani, Mrs. Fatima  female   NaN      0   \n",
       "20                                Fynney, Mr. Joseph J    male  35.0      0   \n",
       "21                               Beesley, Mr. Lawrence    male  34.0      0   \n",
       "22                         McGowan, Miss. Anna \"Annie\"  female  15.0      0   \n",
       "23                        Sloper, Mr. William Thompson    male  28.0      0   \n",
       "24                       Palsson, Miss. Torborg Danira  female   8.0      3   \n",
       "25   Asplund, Mrs. Carl Oscar (Selma Augusta Emilia...  female  38.0      1   \n",
       "26                             Emir, Mr. Farred Chehab    male   NaN      0   \n",
       "27                      Fortune, Mr. Charles Alexander    male  19.0      3   \n",
       "28                       O'Dwyer, Miss. Ellen \"Nellie\"  female   NaN      0   \n",
       "29                                 Todoroff, Mr. Lalio    male   NaN      0   \n",
       "..                                                 ...     ...   ...    ...   \n",
       "861                        Giles, Mr. Frederick Edward    male  21.0      1   \n",
       "862  Swift, Mrs. Frederick Joel (Margaret Welles Ba...  female  48.0      0   \n",
       "863                  Sage, Miss. Dorothy Edith \"Dolly\"  female   NaN      8   \n",
       "864                             Gill, Mr. John William    male  24.0      0   \n",
       "865                           Bystrom, Mrs. (Karolina)  female  42.0      0   \n",
       "866                       Duran y More, Miss. Asuncion  female  27.0      1   \n",
       "867               Roebling, Mr. Washington Augustus II    male  31.0      0   \n",
       "868                        van Melkebeke, Mr. Philemon    male   NaN      0   \n",
       "869                    Johnson, Master. Harold Theodor    male   4.0      1   \n",
       "870                                  Balkic, Mr. Cerin    male  26.0      0   \n",
       "871   Beckwith, Mrs. Richard Leonard (Sallie Monypeny)  female  47.0      1   \n",
       "872                           Carlsson, Mr. Frans Olof    male  33.0      0   \n",
       "873                        Vander Cruyssen, Mr. Victor    male  47.0      0   \n",
       "874              Abelson, Mrs. Samuel (Hannah Wizosky)  female  28.0      1   \n",
       "875                   Najib, Miss. Adele Kiamie \"Jane\"  female  15.0      0   \n",
       "876                      Gustafsson, Mr. Alfred Ossian    male  20.0      0   \n",
       "877                               Petroff, Mr. Nedelio    male  19.0      0   \n",
       "878                                 Laleff, Mr. Kristo    male   NaN      0   \n",
       "879      Potter, Mrs. Thomas Jr (Lily Alexenia Wilson)  female  56.0      0   \n",
       "880       Shelley, Mrs. William (Imanita Parrish Hall)  female  25.0      0   \n",
       "881                                 Markun, Mr. Johann    male  33.0      0   \n",
       "882                       Dahlberg, Miss. Gerda Ulrika  female  22.0      0   \n",
       "883                      Banfield, Mr. Frederick James    male  28.0      0   \n",
       "884                             Sutehall, Mr. Henry Jr    male  25.0      0   \n",
       "885               Rice, Mrs. William (Margaret Norton)  female  39.0      0   \n",
       "886                              Montvila, Rev. Juozas    male  27.0      0   \n",
       "887                       Graham, Miss. Margaret Edith  female  19.0      0   \n",
       "888           Johnston, Miss. Catherine Helen \"Carrie\"  female   NaN      1   \n",
       "889                              Behr, Mr. Karl Howell    male  26.0      0   \n",
       "890                                Dooley, Mr. Patrick    male  32.0      0   \n",
       "\n",
       "     Parch            Ticket      Fare        Cabin Embarked  \n",
       "0        0         A/5 21171    7.2500          NaN        S  \n",
       "1        0          PC 17599   71.2833          C85        C  \n",
       "2        0  STON/O2. 3101282    7.9250          NaN        S  \n",
       "3        0            113803   53.1000         C123        S  \n",
       "4        0            373450    8.0500          NaN        S  \n",
       "5        0            330877    8.4583          NaN        Q  \n",
       "6        0             17463   51.8625          E46        S  \n",
       "7        1            349909   21.0750          NaN        S  \n",
       "8        2            347742   11.1333          NaN        S  \n",
       "9        0            237736   30.0708          NaN        C  \n",
       "10       1           PP 9549   16.7000           G6        S  \n",
       "11       0            113783   26.5500         C103        S  \n",
       "12       0         A/5. 2151    8.0500          NaN        S  \n",
       "13       5            347082   31.2750          NaN        S  \n",
       "14       0            350406    7.8542          NaN        S  \n",
       "15       0            248706   16.0000          NaN        S  \n",
       "16       1            382652   29.1250          NaN        Q  \n",
       "17       0            244373   13.0000          NaN        S  \n",
       "18       0            345763   18.0000          NaN        S  \n",
       "19       0              2649    7.2250          NaN        C  \n",
       "20       0            239865   26.0000          NaN        S  \n",
       "21       0            248698   13.0000          D56        S  \n",
       "22       0            330923    8.0292          NaN        Q  \n",
       "23       0            113788   35.5000           A6        S  \n",
       "24       1            349909   21.0750          NaN        S  \n",
       "25       5            347077   31.3875          NaN        S  \n",
       "26       0              2631    7.2250          NaN        C  \n",
       "27       2             19950  263.0000  C23 C25 C27        S  \n",
       "28       0            330959    7.8792          NaN        Q  \n",
       "29       0            349216    7.8958          NaN        S  \n",
       "..     ...               ...       ...          ...      ...  \n",
       "861      0             28134   11.5000          NaN        S  \n",
       "862      0             17466   25.9292          D17        S  \n",
       "863      2          CA. 2343   69.5500          NaN        S  \n",
       "864      0            233866   13.0000          NaN        S  \n",
       "865      0            236852   13.0000          NaN        S  \n",
       "866      0     SC/PARIS 2149   13.8583          NaN        C  \n",
       "867      0          PC 17590   50.4958          A24        S  \n",
       "868      0            345777    9.5000          NaN        S  \n",
       "869      1            347742   11.1333          NaN        S  \n",
       "870      0            349248    7.8958          NaN        S  \n",
       "871      1             11751   52.5542          D35        S  \n",
       "872      0               695    5.0000  B51 B53 B55        S  \n",
       "873      0            345765    9.0000          NaN        S  \n",
       "874      0         P/PP 3381   24.0000          NaN        C  \n",
       "875      0              2667    7.2250          NaN        C  \n",
       "876      0              7534    9.8458          NaN        S  \n",
       "877      0            349212    7.8958          NaN        S  \n",
       "878      0            349217    7.8958          NaN        S  \n",
       "879      1             11767   83.1583          C50        C  \n",
       "880      1            230433   26.0000          NaN        S  \n",
       "881      0            349257    7.8958          NaN        S  \n",
       "882      0              7552   10.5167          NaN        S  \n",
       "883      0  C.A./SOTON 34068   10.5000          NaN        S  \n",
       "884      0   SOTON/OQ 392076    7.0500          NaN        S  \n",
       "885      5            382652   29.1250          NaN        Q  \n",
       "886      0            211536   13.0000          NaN        S  \n",
       "887      0            112053   30.0000          B42        S  \n",
       "888      2        W./C. 6607   23.4500          NaN        S  \n",
       "889      0            111369   30.0000         C148        C  \n",
       "890      0            370376    7.7500          NaN        Q  \n",
       "\n",
       "[891 rows x 12 columns]"
      ]
     },
     "execution_count": 352,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 12 columns):\n",
      "PassengerId    891 non-null int64\n",
      "Survived       891 non-null int64\n",
      "Pclass         891 non-null int64\n",
      "Name           891 non-null object\n",
      "Sex            891 non-null object\n",
      "Age            714 non-null float64\n",
      "SibSp          891 non-null int64\n",
      "Parch          891 non-null int64\n",
      "Ticket         891 non-null object\n",
      "Fare           891 non-null float64\n",
      "Cabin          204 non-null object\n",
      "Embarked       889 non-null object\n",
      "dtypes: float64(2), int64(5), object(5)\n",
      "memory usage: 83.6+ KB\n"
     ]
    }
   ],
   "source": [
    "train_data_full.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>714.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>446.000000</td>\n",
       "      <td>0.383838</td>\n",
       "      <td>2.308642</td>\n",
       "      <td>29.699118</td>\n",
       "      <td>0.523008</td>\n",
       "      <td>0.381594</td>\n",
       "      <td>32.204208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>257.353842</td>\n",
       "      <td>0.486592</td>\n",
       "      <td>0.836071</td>\n",
       "      <td>14.526497</td>\n",
       "      <td>1.102743</td>\n",
       "      <td>0.806057</td>\n",
       "      <td>49.693429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>223.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>20.125000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.910400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>446.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.454200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>668.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>891.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>512.329200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       PassengerId    Survived      Pclass         Age       SibSp  \\\n",
       "count   891.000000  891.000000  891.000000  714.000000  891.000000   \n",
       "mean    446.000000    0.383838    2.308642   29.699118    0.523008   \n",
       "std     257.353842    0.486592    0.836071   14.526497    1.102743   \n",
       "min       1.000000    0.000000    1.000000    0.420000    0.000000   \n",
       "25%     223.500000    0.000000    2.000000   20.125000    0.000000   \n",
       "50%     446.000000    0.000000    3.000000   28.000000    0.000000   \n",
       "75%     668.500000    1.000000    3.000000   38.000000    1.000000   \n",
       "max     891.000000    1.000000    3.000000   80.000000    8.000000   \n",
       "\n",
       "            Parch        Fare  \n",
       "count  891.000000  891.000000  \n",
       "mean     0.381594   32.204208  \n",
       "std      0.806057   49.693429  \n",
       "min      0.000000    0.000000  \n",
       "25%      0.000000    7.910400  \n",
       "50%      0.000000   14.454200  \n",
       "75%      0.000000   31.000000  \n",
       "max      6.000000  512.329200  "
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_full.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PassengerId\n",
      "891    1\n",
      "293    1\n",
      "304    1\n",
      "303    1\n",
      "302    1\n",
      "301    1\n",
      "300    1\n",
      "299    1\n",
      "298    1\n",
      "297    1\n",
      "296    1\n",
      "295    1\n",
      "294    1\n",
      "292    1\n",
      "306    1\n",
      "291    1\n",
      "290    1\n",
      "289    1\n",
      "288    1\n",
      "287    1\n",
      "286    1\n",
      "285    1\n",
      "284    1\n",
      "283    1\n",
      "282    1\n",
      "281    1\n",
      "305    1\n",
      "307    1\n",
      "279    1\n",
      "321    1\n",
      "      ..\n",
      "561    1\n",
      "560    1\n",
      "584    1\n",
      "585    1\n",
      "586    1\n",
      "587    1\n",
      "610    1\n",
      "609    1\n",
      "608    1\n",
      "607    1\n",
      "606    1\n",
      "605    1\n",
      "604    1\n",
      "603    1\n",
      "602    1\n",
      "601    1\n",
      "600    1\n",
      "599    1\n",
      "598    1\n",
      "597    1\n",
      "596    1\n",
      "595    1\n",
      "594    1\n",
      "593    1\n",
      "592    1\n",
      "591    1\n",
      "590    1\n",
      "589    1\n",
      "588    1\n",
      "1      1\n",
      "Name: PassengerId, Length: 891, dtype: int64\n",
      " \n",
      "Survived\n",
      "0    549\n",
      "1    342\n",
      "Name: Survived, dtype: int64\n",
      " \n",
      "Pclass\n",
      "3    491\n",
      "1    216\n",
      "2    184\n",
      "Name: Pclass, dtype: int64\n",
      " \n",
      "Name\n",
      "Aks, Mrs. Sam (Leah Rosen)                         1\n",
      "Holm, Mr. John Fredrik Alexander                   1\n",
      "Silverthorne, Mr. Spencer Victor                   1\n",
      "Nasser, Mrs. Nicholas (Adele Achem)                1\n",
      "Laroche, Mr. Joseph Philippe Lemercier             1\n",
      "Fortune, Mr. Mark                                  1\n",
      "Olsvigen, Mr. Thor Anderson                        1\n",
      "Elias, Mr. Tannous                                 1\n",
      "Jensen, Mr. Svend Lauritz                          1\n",
      "Olsen, Mr. Karl Siegwart Andreas                   1\n",
      "Laleff, Mr. Kristo                                 1\n",
      "Montvila, Rev. Juozas                              1\n",
      "Bourke, Mrs. John (Catherine)                      1\n",
      "Bonnell, Miss. Elizabeth                           1\n",
      "Ross, Mr. John Hugo                                1\n",
      "O'Connor, Mr. Maurice                              1\n",
      "Sinkkonen, Miss. Anna                              1\n",
      "Jacobsohn, Mr. Sidney Samuel                       1\n",
      "Graham, Miss. Margaret Edith                       1\n",
      "Slocovski, Mr. Selman Francis                      1\n",
      "Skoog, Mr. Wilhelm                                 1\n",
      "McGough, Mr. James Robert                          1\n",
      "Thayer, Mr. John Borland                           1\n",
      "Kelly, Miss. Mary                                  1\n",
      "Healy, Miss. Hanora \"Nora\"                         1\n",
      "Jensen, Mr. Hans Peder                             1\n",
      "Stahelin-Maeglin, Dr. Max                          1\n",
      "Connors, Mr. Patrick                               1\n",
      "Henry, Miss. Delia                                 1\n",
      "Morrow, Mr. Thomas Rowan                           1\n",
      "                                                  ..\n",
      "Salkjelsvik, Miss. Anna Kristine                   1\n",
      "Ivanoff, Mr. Kanio                                 1\n",
      "Yousif, Mr. Wazli                                  1\n",
      "Navratil, Master. Michel M                         1\n",
      "Lam, Mr. Ali                                       1\n",
      "Moubarek, Master. Gerios                           1\n",
      "Madsen, Mr. Fridtjof Arne                          1\n",
      "Johnson, Mr. Alfred                                1\n",
      "Bishop, Mrs. Dickinson H (Helen Walton)            1\n",
      "Robbins, Mr. Victor                                1\n",
      "Webber, Miss. Susan                                1\n",
      "Petroff, Mr. Nedelio                               1\n",
      "Cribb, Mr. John Hatfield                           1\n",
      "Levy, Mr. Rene Jacques                             1\n",
      "West, Miss. Constance Mirium                       1\n",
      "Hoyt, Mrs. Frederick Maxfield (Jane Anne Forby)    1\n",
      "Hold, Mr. Stephen                                  1\n",
      "Boulos, Mr. Hanna                                  1\n",
      "Newell, Miss. Marjorie                             1\n",
      "Frolicher, Miss. Hedwig Margaritha                 1\n",
      "Becker, Master. Richard F                          1\n",
      "Richards, Master. William Rowe                     1\n",
      "Petterson, Mr. Johan Emil                          1\n",
      "Coutts, Master. Eden Leslie \"Neville\"              1\n",
      "Vande Walle, Mr. Nestor Cyriel                     1\n",
      "Eklund, Mr. Hans Linus                             1\n",
      "McCormack, Mr. Thomas Joseph                       1\n",
      "Barbara, Miss. Saiide                              1\n",
      "Moran, Mr. Daniel J                                1\n",
      "Beesley, Mr. Lawrence                              1\n",
      "Name: Name, Length: 891, dtype: int64\n",
      " \n",
      "Sex\n",
      "male      577\n",
      "female    314\n",
      "Name: Sex, dtype: int64\n",
      " \n",
      "Age\n",
      "24.00    30\n",
      "22.00    27\n",
      "18.00    26\n",
      "19.00    25\n",
      "30.00    25\n",
      "28.00    25\n",
      "21.00    24\n",
      "25.00    23\n",
      "36.00    22\n",
      "29.00    20\n",
      "32.00    18\n",
      "27.00    18\n",
      "35.00    18\n",
      "26.00    18\n",
      "16.00    17\n",
      "31.00    17\n",
      "20.00    15\n",
      "33.00    15\n",
      "23.00    15\n",
      "34.00    15\n",
      "39.00    14\n",
      "17.00    13\n",
      "42.00    13\n",
      "40.00    13\n",
      "45.00    12\n",
      "38.00    11\n",
      "50.00    10\n",
      "2.00     10\n",
      "4.00     10\n",
      "47.00     9\n",
      "         ..\n",
      "71.00     2\n",
      "59.00     2\n",
      "63.00     2\n",
      "0.83      2\n",
      "30.50     2\n",
      "70.00     2\n",
      "57.00     2\n",
      "0.75      2\n",
      "13.00     2\n",
      "10.00     2\n",
      "64.00     2\n",
      "40.50     2\n",
      "32.50     2\n",
      "45.50     2\n",
      "20.50     1\n",
      "24.50     1\n",
      "0.67      1\n",
      "14.50     1\n",
      "0.92      1\n",
      "74.00     1\n",
      "34.50     1\n",
      "80.00     1\n",
      "12.00     1\n",
      "36.50     1\n",
      "53.00     1\n",
      "55.50     1\n",
      "70.50     1\n",
      "66.00     1\n",
      "23.50     1\n",
      "0.42      1\n",
      "Name: Age, Length: 88, dtype: int64\n",
      " \n",
      "SibSp\n",
      "0    608\n",
      "1    209\n",
      "2     28\n",
      "4     18\n",
      "3     16\n",
      "8      7\n",
      "5      5\n",
      "Name: SibSp, dtype: int64\n",
      " \n",
      "Parch\n",
      "0    678\n",
      "1    118\n",
      "2     80\n",
      "5      5\n",
      "3      5\n",
      "4      4\n",
      "6      1\n",
      "Name: Parch, dtype: int64\n",
      " \n",
      "Ticket\n",
      "347082               7\n",
      "CA. 2343             7\n",
      "1601                 7\n",
      "3101295              6\n",
      "347088               6\n",
      "CA 2144              6\n",
      "S.O.C. 14879         5\n",
      "382652               5\n",
      "113760               4\n",
      "349909               4\n",
      "19950                4\n",
      "2666                 4\n",
      "W./C. 6608           4\n",
      "17421                4\n",
      "113781               4\n",
      "LINE                 4\n",
      "347077               4\n",
      "PC 17757             4\n",
      "4133                 4\n",
      "248727               3\n",
      "C.A. 31921           3\n",
      "PC 17582             3\n",
      "PC 17760             3\n",
      "110413               3\n",
      "C.A. 34651           3\n",
      "29106                3\n",
      "371110               3\n",
      "13502                3\n",
      "239853               3\n",
      "24160                3\n",
      "                    ..\n",
      "14311                1\n",
      "19988                1\n",
      "330958               1\n",
      "11755                1\n",
      "231945               1\n",
      "C.A. 5547            1\n",
      "SOTON/OQ 392086      1\n",
      "349910               1\n",
      "364500               1\n",
      "2631                 1\n",
      "113509               1\n",
      "7545                 1\n",
      "STON/O 2. 3101274    1\n",
      "334912               1\n",
      "11771                1\n",
      "113767               1\n",
      "Fa 265302            1\n",
      "PC 17609             1\n",
      "113043               1\n",
      "9234                 1\n",
      "367231               1\n",
      "349248               1\n",
      "330979               1\n",
      "F.C.C. 13528         1\n",
      "315084               1\n",
      "SOTON/OQ 392090      1\n",
      "218629               1\n",
      "STON/O 2. 3101293    1\n",
      "2641                 1\n",
      "113056               1\n",
      "Name: Ticket, Length: 681, dtype: int64\n",
      " \n",
      "Fare\n",
      "8.0500      43\n",
      "13.0000     42\n",
      "7.8958      38\n",
      "7.7500      34\n",
      "26.0000     31\n",
      "10.5000     24\n",
      "7.9250      18\n",
      "7.7750      16\n",
      "26.5500     15\n",
      "0.0000      15\n",
      "7.2292      15\n",
      "7.8542      13\n",
      "8.6625      13\n",
      "7.2500      13\n",
      "7.2250      12\n",
      "16.1000      9\n",
      "9.5000       9\n",
      "24.1500      8\n",
      "15.5000      8\n",
      "56.4958      7\n",
      "52.0000      7\n",
      "14.5000      7\n",
      "14.4542      7\n",
      "69.5500      7\n",
      "7.0500       7\n",
      "31.2750      7\n",
      "46.9000      6\n",
      "30.0000      6\n",
      "7.7958       6\n",
      "39.6875      6\n",
      "            ..\n",
      "7.1417       1\n",
      "42.4000      1\n",
      "211.5000     1\n",
      "12.2750      1\n",
      "61.1750      1\n",
      "8.4333       1\n",
      "51.4792      1\n",
      "7.8875       1\n",
      "8.6833       1\n",
      "7.5208       1\n",
      "34.6542      1\n",
      "28.7125      1\n",
      "25.5875      1\n",
      "7.7292       1\n",
      "12.2875      1\n",
      "8.6542       1\n",
      "8.7125       1\n",
      "61.3792      1\n",
      "6.9500       1\n",
      "9.8417       1\n",
      "8.3000       1\n",
      "13.7917      1\n",
      "9.4750       1\n",
      "13.4167      1\n",
      "26.3875      1\n",
      "8.4583       1\n",
      "9.8375       1\n",
      "8.3625       1\n",
      "14.1083      1\n",
      "17.4000      1\n",
      "Name: Fare, Length: 248, dtype: int64\n",
      " \n",
      "Cabin\n",
      "B96 B98        4\n",
      "G6             4\n",
      "C23 C25 C27    4\n",
      "E101           3\n",
      "F2             3\n",
      "D              3\n",
      "F33            3\n",
      "C22 C26        3\n",
      "C123           2\n",
      "E44            2\n",
      "B20            2\n",
      "B77            2\n",
      "E33            2\n",
      "C83            2\n",
      "E121           2\n",
      "C126           2\n",
      "B51 B53 B55    2\n",
      "F G73          2\n",
      "E8             2\n",
      "C78            2\n",
      "C68            2\n",
      "B49            2\n",
      "B35            2\n",
      "D17            2\n",
      "C93            2\n",
      "D20            2\n",
      "E24            2\n",
      "D26            2\n",
      "B18            2\n",
      "C125           2\n",
      "              ..\n",
      "B30            1\n",
      "B101           1\n",
      "B4             1\n",
      "T              1\n",
      "E77            1\n",
      "A26            1\n",
      "A32            1\n",
      "D28            1\n",
      "C95            1\n",
      "C46            1\n",
      "D9             1\n",
      "E31            1\n",
      "A24            1\n",
      "C91            1\n",
      "B37            1\n",
      "C85            1\n",
      "F38            1\n",
      "C32            1\n",
      "E63            1\n",
      "B86            1\n",
      "B82 B84        1\n",
      "D45            1\n",
      "C82            1\n",
      "B94            1\n",
      "C101           1\n",
      "C90            1\n",
      "C86            1\n",
      "D30            1\n",
      "E50            1\n",
      "C62 C64        1\n",
      "Name: Cabin, Length: 147, dtype: int64\n",
      " \n",
      "Embarked\n",
      "S    644\n",
      "C    168\n",
      "Q     77\n",
      "Name: Embarked, dtype: int64\n",
      " \n"
     ]
    }
   ],
   "source": [
    "col_names = list(train_data_full)\n",
    "\n",
    "for i in range(len(col_names)):\n",
    "    col_name = col_names[i]\n",
    "    print(col_name)\n",
    "    string_data = str(col_name)\n",
    "    print(train_data_full[string_data].value_counts())\n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separate Labels and Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = train_data_full.drop(\"Survived\", axis = 1)\n",
    "train_labels = train_data_full[\"Survived\"].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>714.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>446.000000</td>\n",
       "      <td>2.308642</td>\n",
       "      <td>29.699118</td>\n",
       "      <td>0.523008</td>\n",
       "      <td>0.381594</td>\n",
       "      <td>32.204208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>257.353842</td>\n",
       "      <td>0.836071</td>\n",
       "      <td>14.526497</td>\n",
       "      <td>1.102743</td>\n",
       "      <td>0.806057</td>\n",
       "      <td>49.693429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>223.500000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>20.125000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.910400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>446.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.454200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>668.500000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>891.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>512.329200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       PassengerId      Pclass         Age       SibSp       Parch        Fare\n",
       "count   891.000000  891.000000  714.000000  891.000000  891.000000  891.000000\n",
       "mean    446.000000    2.308642   29.699118    0.523008    0.381594   32.204208\n",
       "std     257.353842    0.836071   14.526497    1.102743    0.806057   49.693429\n",
       "min       1.000000    1.000000    0.420000    0.000000    0.000000    0.000000\n",
       "25%     223.500000    2.000000   20.125000    0.000000    0.000000    7.910400\n",
       "50%     446.000000    3.000000   28.000000    0.000000    0.000000   14.454200\n",
       "75%     668.500000    3.000000   38.000000    1.000000    0.000000   31.000000\n",
       "max     891.000000    3.000000   80.000000    8.000000    6.000000  512.329200"
      ]
     },
     "execution_count": 357,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drop the PassengerID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "passenger_id_train = train_data[\"PassengerId\"].copy()\n",
    "train_data = train_data.drop(\"PassengerId\", axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "passenger_id_test = test_data[\"PassengerId\"].copy()\n",
    "test_data = test_data.drop(\"PassengerId\", axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline\n",
    "\n",
    "- 1-hot Enconder\n",
    "- Normalizer\n",
    "- Imputer (to deal with the NaN values)\n",
    "- Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Background magic\n",
    "\n",
    "# The CategoricalEncoder class will allow us to convert categorical attributes to one-hot vectors.\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.utils import check_array\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from scipy import sparse\n",
    "\n",
    "class CategoricalEncoder(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Encode categorical features as a numeric array.\n",
    "    The input to this transformer should be a matrix of integers or strings,\n",
    "    denoting the values taken on by categorical (discrete) features.\n",
    "    The features can be encoded using a one-hot aka one-of-K scheme\n",
    "    (``encoding='onehot'``, the default) or converted to ordinal integers\n",
    "    (``encoding='ordinal'``).\n",
    "    This encoding is needed for feeding categorical data to many scikit-learn\n",
    "    estimators, notably linear models and SVMs with the standard kernels.\n",
    "    Read more in the :ref:`User Guide <preprocessing_categorical_features>`.\n",
    "    Parameters\n",
    "    ----------\n",
    "    encoding : str, 'onehot', 'onehot-dense' or 'ordinal'\n",
    "        The type of encoding to use (default is 'onehot'):\n",
    "        - 'onehot': encode the features using a one-hot aka one-of-K scheme\n",
    "          (or also called 'dummy' encoding). This creates a binary column for\n",
    "          each category and returns a sparse matrix.\n",
    "        - 'onehot-dense': the same as 'onehot' but returns a dense array\n",
    "          instead of a sparse matrix.\n",
    "        - 'ordinal': encode the features as ordinal integers. This results in\n",
    "          a single column of integers (0 to n_categories - 1) per feature.\n",
    "    categories : 'auto' or a list of lists/arrays of values.\n",
    "        Categories (unique values) per feature:\n",
    "        - 'auto' : Determine categories automatically from the training data.\n",
    "        - list : ``categories[i]`` holds the categories expected in the ith\n",
    "          column. The passed categories are sorted before encoding the data\n",
    "          (used categories can be found in the ``categories_`` attribute).\n",
    "    dtype : number type, default np.float64\n",
    "        Desired dtype of output.\n",
    "    handle_unknown : 'error' (default) or 'ignore'\n",
    "        Whether to raise an error or ignore if a unknown categorical feature is\n",
    "        present during transform (default is to raise). When this is parameter\n",
    "        is set to 'ignore' and an unknown category is encountered during\n",
    "        transform, the resulting one-hot encoded columns for this feature\n",
    "        will be all zeros.\n",
    "        Ignoring unknown categories is not supported for\n",
    "        ``encoding='ordinal'``.\n",
    "    Attributes\n",
    "    ----------\n",
    "    categories_ : list of arrays\n",
    "        The categories of each feature determined during fitting. When\n",
    "        categories were specified manually, this holds the sorted categories\n",
    "        (in order corresponding with output of `transform`).\n",
    "    Examples\n",
    "    --------\n",
    "    Given a dataset with three features and two samples, we let the encoder\n",
    "    find the maximum value per feature and transform the data to a binary\n",
    "    one-hot encoding.\n",
    "    >>> from sklearn.preprocessing import CategoricalEncoder\n",
    "    >>> enc = CategoricalEncoder(handle_unknown='ignore')\n",
    "    >>> enc.fit([[0, 0, 3], [1, 1, 0], [0, 2, 1], [1, 0, 2]])\n",
    "    ... # doctest: +ELLIPSIS\n",
    "    CategoricalEncoder(categories='auto', dtype=<... 'numpy.float64'>,\n",
    "              encoding='onehot', handle_unknown='ignore')\n",
    "    >>> enc.transform([[0, 1, 1], [1, 0, 4]]).toarray()\n",
    "    array([[ 1.,  0.,  0.,  1.,  0.,  0.,  1.,  0.,  0.],\n",
    "           [ 0.,  1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.]])\n",
    "    See also\n",
    "    --------\n",
    "    sklearn.preprocessing.OneHotEncoder : performs a one-hot encoding of\n",
    "      integer ordinal features. The ``OneHotEncoder assumes`` that input\n",
    "      features take on values in the range ``[0, max(feature)]`` instead of\n",
    "      using the unique values.\n",
    "    sklearn.feature_extraction.DictVectorizer : performs a one-hot encoding of\n",
    "      dictionary items (also handles string-valued features).\n",
    "    sklearn.feature_extraction.FeatureHasher : performs an approximate one-hot\n",
    "      encoding of dictionary items or strings.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, encoding='onehot', categories='auto', dtype=np.float64,\n",
    "                 handle_unknown='error'):\n",
    "        self.encoding = encoding\n",
    "        self.categories = categories\n",
    "        self.dtype = dtype\n",
    "        self.handle_unknown = handle_unknown\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"Fit the CategoricalEncoder to X.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape [n_samples, n_feature]\n",
    "            The data to determine the categories of each feature.\n",
    "        Returns\n",
    "        -------\n",
    "        self\n",
    "        \"\"\"\n",
    "\n",
    "        if self.encoding not in ['onehot', 'onehot-dense', 'ordinal']:\n",
    "            template = (\"encoding should be either 'onehot', 'onehot-dense' \"\n",
    "                        \"or 'ordinal', got %s\")\n",
    "            raise ValueError(template % self.handle_unknown)\n",
    "\n",
    "        if self.handle_unknown not in ['error', 'ignore']:\n",
    "            template = (\"handle_unknown should be either 'error' or \"\n",
    "                        \"'ignore', got %s\")\n",
    "            raise ValueError(template % self.handle_unknown)\n",
    "\n",
    "        if self.encoding == 'ordinal' and self.handle_unknown == 'ignore':\n",
    "            raise ValueError(\"handle_unknown='ignore' is not supported for\"\n",
    "                             \" encoding='ordinal'\")\n",
    "\n",
    "        X = check_array(X, dtype=np.object, accept_sparse='csc', copy=True)\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        self._label_encoders_ = [LabelEncoder() for _ in range(n_features)]\n",
    "\n",
    "        for i in range(n_features):\n",
    "            le = self._label_encoders_[i]\n",
    "            Xi = X[:, i]\n",
    "            if self.categories == 'auto':\n",
    "                le.fit(Xi)\n",
    "            else:\n",
    "                valid_mask = np.in1d(Xi, self.categories[i])\n",
    "                if not np.all(valid_mask):\n",
    "                    if self.handle_unknown == 'error':\n",
    "                        diff = np.unique(Xi[~valid_mask])\n",
    "                        msg = (\"Found unknown categories {0} in column {1}\"\n",
    "                               \" during fit\".format(diff, i))\n",
    "                        raise ValueError(msg)\n",
    "                le.classes_ = np.array(np.sort(self.categories[i]))\n",
    "\n",
    "        self.categories_ = [le.classes_ for le in self._label_encoders_]\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"Transform X using one-hot encoding.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape [n_samples, n_features]\n",
    "            The data to encode.\n",
    "        Returns\n",
    "        -------\n",
    "        X_out : sparse matrix or a 2-d array\n",
    "            Transformed input.\n",
    "        \"\"\"\n",
    "        X = check_array(X, accept_sparse='csc', dtype=np.object, copy=True)\n",
    "        n_samples, n_features = X.shape\n",
    "        X_int = np.zeros_like(X, dtype=np.int)\n",
    "        X_mask = np.ones_like(X, dtype=np.bool)\n",
    "\n",
    "        for i in range(n_features):\n",
    "            valid_mask = np.in1d(X[:, i], self.categories_[i])\n",
    "\n",
    "            if not np.all(valid_mask):\n",
    "                if self.handle_unknown == 'error':\n",
    "                    diff = np.unique(X[~valid_mask, i])\n",
    "                    msg = (\"Found unknown categories {0} in column {1}\"\n",
    "                           \" during transform\".format(diff, i))\n",
    "                    raise ValueError(msg)\n",
    "                else:\n",
    "                    # Set the problematic rows to an acceptable value and\n",
    "                    # continue `The rows are marked `X_mask` and will be\n",
    "                    # removed later.\n",
    "                    X_mask[:, i] = valid_mask\n",
    "                    X[:, i][~valid_mask] = self.categories_[i][0]\n",
    "            X_int[:, i] = self._label_encoders_[i].transform(X[:, i])\n",
    "\n",
    "        if self.encoding == 'ordinal':\n",
    "            return X_int.astype(self.dtype, copy=False)\n",
    "\n",
    "        mask = X_mask.ravel()\n",
    "        n_values = [cats.shape[0] for cats in self.categories_]\n",
    "        n_values = np.array([0] + n_values)\n",
    "        indices = np.cumsum(n_values)\n",
    "\n",
    "        column_indices = (X_int + indices[:-1]).ravel()[mask]\n",
    "        row_indices = np.repeat(np.arange(n_samples, dtype=np.int32),\n",
    "                                n_features)[mask]\n",
    "        data = np.ones(n_samples * n_features)[mask]\n",
    "\n",
    "        out = sparse.csc_matrix((data, (row_indices, column_indices)),\n",
    "                                shape=(n_samples, indices[-1]),\n",
    "                                dtype=self.dtype).tocsr()\n",
    "        if self.encoding == 'onehot-dense':\n",
    "            return out.toarray()\n",
    "        else:\n",
    "            return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# A class to select numerical or categorical columns \n",
    "# since Scikit-Learn doesn't handle DataFrames yet\n",
    "class DataFrameSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, attribute_names):\n",
    "        self.attribute_names = attribute_names\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return X[self.attribute_names]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create an imputer for the categorical values as the normal one would not work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Inspired from stackoverflow.com/questions/25239958\n",
    "class MostFrequentImputer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        self.most_frequent = pd.Series([X[c].value_counts().index[0] for c in X],\n",
    "                                       index=X.columns)\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        return X.fillna(self.most_frequent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#############\n",
    "# PIPELINE #\n",
    "############\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.utils import check_array\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from scipy import sparse\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "# Let's create an imputer to deal with the missing values\n",
    "\n",
    "imputer = Imputer(strategy = \"median\")\n",
    "\n",
    "# Let's take care of the numerical data\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "        (\"select_numeric\", DataFrameSelector([\"Age\", \"SibSp\", \"Parch\", \"Fare\"])),\n",
    "        (\"imputer\", Imputer(strategy=\"median\")),\n",
    "        ('std_scaler', MinMaxScaler()),\n",
    "    ])\n",
    "\n",
    "# Let's take care of the categorical data\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "        (\"select_cat\", DataFrameSelector([\"Pclass\", \"Sex\", \"Embarked\"])),\n",
    "        (\"imputer\", MostFrequentImputer()),\n",
    "        (\"cat_encoder\", CategoricalEncoder(encoding='onehot-dense')),\n",
    "    ])\n",
    "\n",
    "# Let's join the numerical and categorical data using the union\n",
    "\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "preprocess_pipeline = FeatureUnion(transformer_list=[\n",
    "        (\"num_pipeline\", num_pipeline),\n",
    "        (\"cat_pipeline\", cat_pipeline),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ACTIVATE THE PIPELINE!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.27117366, 0.125     , 0.        , ..., 0.        , 0.        ,\n",
       "        1.        ],\n",
       "       [0.4722292 , 0.125     , 0.        , ..., 1.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.32143755, 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        1.        ],\n",
       "       ...,\n",
       "       [0.34656949, 0.125     , 0.33333333, ..., 0.        , 0.        ,\n",
       "        1.        ],\n",
       "       [0.32143755, 0.        , 0.        , ..., 1.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.39683338, 0.        , 0.        , ..., 0.        , 1.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 360,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = preprocess_pipeline.fit_transform(train_data)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test = preprocess_pipeline.transform(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC # DONE\n",
    "from scipy.stats import reciprocal, uniform\n",
    "from sklearn.ensemble import RandomForestClassifier # DONE\n",
    "from sklearn.ensemble import AdaBoostClassifier # DONE\n",
    "from sklearn.ensemble import ExtraTreesClassifier # DONE\n",
    "from sklearn.ensemble import GradientBoostingClassifier # DONE\n",
    "from sklearn.linear_model import LogisticRegression # DONE\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "[CV] C=697469.1855978617, gamma=0.00026958559022082775 ...............\n",
      "[CV]  C=697469.1855978617, gamma=0.00026958559022082775, total=   0.1s\n",
      "[CV] C=697469.1855978617, gamma=0.00026958559022082775 ...............\n",
      "[CV]  C=697469.1855978617, gamma=0.00026958559022082775, total=   0.1s\n",
      "[CV] C=697469.1855978617, gamma=0.00026958559022082775 ...............\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=697469.1855978617, gamma=0.00026958559022082775, total=   0.1s\n",
      "[CV] C=227851.4535642031, gamma=0.005709181441858974 .................\n",
      "[CV] .. C=227851.4535642031, gamma=0.005709181441858974, total=   0.2s\n",
      "[CV] C=227851.4535642031, gamma=0.005709181441858974 .................\n",
      "[CV] .. C=227851.4535642031, gamma=0.005709181441858974, total=   0.3s\n",
      "[CV] C=227851.4535642031, gamma=0.005709181441858974 .................\n",
      "[CV] .. C=227851.4535642031, gamma=0.005709181441858974, total=   0.2s\n",
      "[CV] C=720468.9697855631, gamma=0.00130476500683914 ..................\n",
      "[CV] ... C=720468.9697855631, gamma=0.00130476500683914, total=   0.2s\n",
      "[CV] C=720468.9697855631, gamma=0.00130476500683914 ..................\n",
      "[CV] ... C=720468.9697855631, gamma=0.00130476500683914, total=   0.2s\n",
      "[CV] C=720468.9697855631, gamma=0.00130476500683914 ..................\n",
      "[CV] ... C=720468.9697855631, gamma=0.00130476500683914, total=   0.2s\n",
      "[CV] C=981764.1983846155, gamma=0.02655514596421073 ..................\n",
      "[CV] ... C=981764.1983846155, gamma=0.02655514596421073, total=   3.4s\n",
      "[CV] C=981764.1983846155, gamma=0.02655514596421073 ..................\n",
      "[CV] ... C=981764.1983846155, gamma=0.02655514596421073, total=   2.1s\n",
      "[CV] C=981764.1983846155, gamma=0.02655514596421073 ..................\n",
      "[CV] ... C=981764.1983846155, gamma=0.02655514596421073, total=   1.0s\n",
      "[CV] C=481931.9014843609, gamma=0.0009132456052661993 ................\n",
      "[CV] . C=481931.9014843609, gamma=0.0009132456052661993, total=   0.1s\n",
      "[CV] C=481931.9014843609, gamma=0.0009132456052661993 ................\n",
      "[CV] . C=481931.9014843609, gamma=0.0009132456052661993, total=   0.1s\n",
      "[CV] C=481931.9014843609, gamma=0.0009132456052661993 ................\n",
      "[CV] . C=481931.9014843609, gamma=0.0009132456052661993, total=   0.1s\n",
      "[CV] C=344178.01615086943, gamma=0.044182322046543765 ................\n",
      "[CV] . C=344178.01615086943, gamma=0.044182322046543765, total=   0.8s\n",
      "[CV] C=344178.01615086943, gamma=0.044182322046543765 ................\n",
      "[CV] . C=344178.01615086943, gamma=0.044182322046543765, total=   0.9s\n",
      "[CV] C=344178.01615086943, gamma=0.044182322046543765 ................\n",
      "[CV] . C=344178.01615086943, gamma=0.044182322046543765, total=   1.0s\n",
      "[CV] C=439572.2446796244, gamma=1.987876881680064e-05 ................\n",
      "[CV] . C=439572.2446796244, gamma=1.987876881680064e-05, total=   0.0s\n",
      "[CV] C=439572.2446796244, gamma=1.987876881680064e-05 ................\n",
      "[CV] . C=439572.2446796244, gamma=1.987876881680064e-05, total=   0.0s\n",
      "[CV] C=439572.2446796244, gamma=1.987876881680064e-05 ................\n",
      "[CV] . C=439572.2446796244, gamma=1.987876881680064e-05, total=   0.0s\n",
      "[CV] C=399044.2553304314, gamma=0.048975291395490256 .................\n",
      "[CV] .. C=399044.2553304314, gamma=0.048975291395490256, total=   1.5s\n",
      "[CV] C=399044.2553304314, gamma=0.048975291395490256 .................\n",
      "[CV] .. C=399044.2553304314, gamma=0.048975291395490256, total=   1.2s\n",
      "[CV] C=399044.2553304314, gamma=0.048975291395490256 .................\n",
      "[CV] .. C=399044.2553304314, gamma=0.048975291395490256, total=   2.1s\n",
      "[CV] C=183491.73045349997, gamma=7.538045954647016e-05 ...............\n",
      "[CV]  C=183491.73045349997, gamma=7.538045954647016e-05, total=   0.0s\n",
      "[CV] C=183491.73045349997, gamma=7.538045954647016e-05 ...............\n",
      "[CV]  C=183491.73045349997, gamma=7.538045954647016e-05, total=   0.0s\n",
      "[CV] C=183491.73045349997, gamma=7.538045954647016e-05 ...............\n",
      "[CV]  C=183491.73045349997, gamma=7.538045954647016e-05, total=   0.1s\n",
      "[CV] C=532551.3738418383, gamma=0.004561817800145397 .................\n",
      "[CV] .. C=532551.3738418383, gamma=0.004561817800145397, total=   0.2s\n",
      "[CV] C=532551.3738418383, gamma=0.004561817800145397 .................\n",
      "[CV] .. C=532551.3738418383, gamma=0.004561817800145397, total=   0.4s\n",
      "[CV] C=532551.3738418383, gamma=0.004561817800145397 .................\n",
      "[CV] .. C=532551.3738418383, gamma=0.004561817800145397, total=   0.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  30 out of  30 | elapsed:   17.6s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=None, error_score='raise',\n",
       "          estimator=SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=42, shrinking=True,\n",
       "  tol=0.001, verbose=False),\n",
       "          fit_params=None, iid=True, n_iter=10, n_jobs=1,\n",
       "          param_distributions={'gamma': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7fb75fab0550>, 'C': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7fb75fab0b38>},\n",
       "          pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "          return_train_score='warn', scoring=None, verbose=2)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc_clf = SVC(random_state = 42)\n",
    "\n",
    "#kernel = \n",
    "#gamma =\n",
    "#C =\n",
    "\n",
    "#param_distributions = {'C':[0.1, 1, 10, 100],'gamma':[1,0.1,0.001,0.0001]}\n",
    "#gird_search_cv_svc = GridSearchCV(svc_clf, param_distributions, cv = 4, scoring='roc_auc', refit = True, verbose = 2,\n",
    "#                                 n_jobs = -1)\n",
    "#gird_search_cv_svc.fit(X_train, train_labels)\n",
    "\n",
    "param_distributions = {\"gamma\": reciprocal(0.00001, 1), \"C\": uniform(1000, 1000000)}\n",
    "\n",
    "rnd_search_cv_svc = RandomizedSearchCV(svc_clf, param_distributions, n_iter=10, verbose=2, n_jobs = 1)\n",
    "rnd_search_cv_svc.fit(X_train, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 227851.4535642031, 'gamma': 0.005709181441858974}\n",
      "SVC(C=227851.4535642031, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.005709181441858974,\n",
      "  kernel='rbf', max_iter=-1, probability=False, random_state=42,\n",
      "  shrinking=True, tol=0.001, verbose=False)\n"
     ]
    }
   ],
   "source": [
    "print(rnd_search_cv_svc.best_params_)\n",
    "\n",
    "print(rnd_search_cv_svc.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8361391694725028"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc_randomized = rnd_search_cv_svc.best_estimator_\n",
    "\n",
    "y_pred_svc_randomized = svc_randomized.predict(X_train)\n",
    "accuracy_score(train_labels, y_pred_svc_randomized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[523  26]\n",
      " [120 222]]\n",
      "Precision:  0.8951612903225806\n",
      "Recall:  0.6491228070175439\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(train_labels, y_pred_svc_randomized))\n",
    "\n",
    "print(\"Precision: \", precision_score(train_labels, y_pred_svc_randomized))\n",
    "\n",
    "print(\"Recall: \", recall_score(train_labels, y_pred_svc_randomized))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Classifer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 10 candidates, totalling 40 fits\n",
      "[CV] oob_score=False, n_estimators=10, min_samples_split=0.5, min_samples_leaf=0.1, max_leaf_nodes=100, max_features=0.5, max_depth=20 \n",
      "[CV] oob_score=False, n_estimators=10, min_samples_split=0.5, min_samples_leaf=0.1, max_leaf_nodes=100, max_features=0.5, max_depth=20 \n",
      "[CV] oob_score=False, n_estimators=10, min_samples_split=0.5, min_samples_leaf=0.1, max_leaf_nodes=100, max_features=0.5, max_depth=20 \n",
      "[CV] oob_score=False, n_estimators=10, min_samples_split=0.5, min_samples_leaf=0.1, max_leaf_nodes=100, max_features=0.5, max_depth=20 \n",
      "[CV]  oob_score=False, n_estimators=10, min_samples_split=0.5, min_samples_leaf=0.1, max_leaf_nodes=100, max_features=0.5, max_depth=20, total=   0.0s\n",
      "[CV]  oob_score=False, n_estimators=10, min_samples_split=0.5, min_samples_leaf=0.1, max_leaf_nodes=100, max_features=0.5, max_depth=20, total=   0.0s\n",
      "[CV] oob_score=False, n_estimators=200, min_samples_split=0.1, min_samples_leaf=0.5, max_leaf_nodes=2, max_features=0.1, max_depth=2 \n",
      "[CV]  oob_score=False, n_estimators=10, min_samples_split=0.5, min_samples_leaf=0.1, max_leaf_nodes=100, max_features=0.5, max_depth=20, total=   0.0s\n",
      "[CV]  oob_score=False, n_estimators=10, min_samples_split=0.5, min_samples_leaf=0.1, max_leaf_nodes=100, max_features=0.5, max_depth=20, total=   0.1s\n",
      "[CV] oob_score=False, n_estimators=200, min_samples_split=0.1, min_samples_leaf=0.5, max_leaf_nodes=2, max_features=0.1, max_depth=2 \n",
      "[CV] oob_score=False, n_estimators=200, min_samples_split=0.1, min_samples_leaf=0.5, max_leaf_nodes=2, max_features=0.1, max_depth=2 \n",
      "[CV] oob_score=False, n_estimators=200, min_samples_split=0.1, min_samples_leaf=0.5, max_leaf_nodes=2, max_features=0.1, max_depth=2 \n",
      "[CV]  oob_score=False, n_estimators=200, min_samples_split=0.1, min_samples_leaf=0.5, max_leaf_nodes=2, max_features=0.1, max_depth=2, total=   0.4s\n",
      "[CV] oob_score=True, n_estimators=200, min_samples_split=0.5, min_samples_leaf=0.1, max_leaf_nodes=100, max_features=0.5, max_depth=20 \n",
      "[CV]  oob_score=False, n_estimators=200, min_samples_split=0.1, min_samples_leaf=0.5, max_leaf_nodes=2, max_features=0.1, max_depth=2, total=   0.4s\n",
      "[CV]  oob_score=False, n_estimators=200, min_samples_split=0.1, min_samples_leaf=0.5, max_leaf_nodes=2, max_features=0.1, max_depth=2, total=   0.4s\n",
      "[CV] oob_score=True, n_estimators=200, min_samples_split=0.5, min_samples_leaf=0.1, max_leaf_nodes=10, max_features=0.5, max_depth=10 \n",
      "[CV] oob_score=False, n_estimators=200, min_samples_split=0.1, min_samples_leaf=0.1, max_leaf_nodes=2, max_features=0.5, max_depth=10 \n",
      "[CV]  oob_score=False, n_estimators=200, min_samples_split=0.1, min_samples_leaf=0.5, max_leaf_nodes=2, max_features=0.1, max_depth=2, total=   0.6s\n",
      "[CV] oob_score=True, n_estimators=10, min_samples_split=0.1, min_samples_leaf=0.1, max_leaf_nodes=100, max_features=0.1, max_depth=10 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/isaac/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/home/isaac/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  oob_score=True, n_estimators=10, min_samples_split=0.1, min_samples_leaf=0.1, max_leaf_nodes=100, max_features=0.1, max_depth=10, total=   0.1s\n",
      "[CV] oob_score=True, n_estimators=10, min_samples_split=0.1, min_samples_leaf=0.1, max_leaf_nodes=100, max_features=0.1, max_depth=10 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/isaac/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/home/isaac/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  oob_score=True, n_estimators=10, min_samples_split=0.1, min_samples_leaf=0.1, max_leaf_nodes=100, max_features=0.1, max_depth=10, total=   0.1s\n",
      "[CV] oob_score=True, n_estimators=10, min_samples_split=0.1, min_samples_leaf=0.1, max_leaf_nodes=100, max_features=0.1, max_depth=10 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/isaac/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/home/isaac/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  oob_score=True, n_estimators=10, min_samples_split=0.1, min_samples_leaf=0.1, max_leaf_nodes=100, max_features=0.1, max_depth=10, total=   0.1s\n",
      "[CV] oob_score=True, n_estimators=10, min_samples_split=0.1, min_samples_leaf=0.1, max_leaf_nodes=100, max_features=0.1, max_depth=10 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/isaac/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/home/isaac/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  oob_score=True, n_estimators=200, min_samples_split=0.5, min_samples_leaf=0.1, max_leaf_nodes=100, max_features=0.5, max_depth=20, total=   0.5s\n",
      "[CV] oob_score=True, n_estimators=200, min_samples_split=0.5, min_samples_leaf=0.1, max_leaf_nodes=100, max_features=0.5, max_depth=20 \n",
      "[CV]  oob_score=True, n_estimators=10, min_samples_split=0.1, min_samples_leaf=0.1, max_leaf_nodes=100, max_features=0.1, max_depth=10, total=   0.1s\n",
      "[CV] oob_score=False, n_estimators=200, min_samples_split=0.5, min_samples_leaf=0.5, max_leaf_nodes=10, max_features=0.5, max_depth=2 \n",
      "[CV]  oob_score=True, n_estimators=200, min_samples_split=0.5, min_samples_leaf=0.1, max_leaf_nodes=10, max_features=0.5, max_depth=10, total=   0.6s\n",
      "[CV] oob_score=True, n_estimators=200, min_samples_split=0.5, min_samples_leaf=0.1, max_leaf_nodes=10, max_features=0.5, max_depth=10 \n",
      "[CV]  oob_score=False, n_estimators=200, min_samples_split=0.1, min_samples_leaf=0.1, max_leaf_nodes=2, max_features=0.5, max_depth=10, total=   0.6s\n",
      "[CV] oob_score=False, n_estimators=200, min_samples_split=0.1, min_samples_leaf=0.1, max_leaf_nodes=2, max_features=0.5, max_depth=10 \n",
      "[CV]  oob_score=True, n_estimators=200, min_samples_split=0.5, min_samples_leaf=0.1, max_leaf_nodes=100, max_features=0.5, max_depth=20, total=   0.5s\n",
      "[CV] oob_score=True, n_estimators=200, min_samples_split=0.5, min_samples_leaf=0.1, max_leaf_nodes=100, max_features=0.5, max_depth=20 \n",
      "[CV]  oob_score=False, n_estimators=200, min_samples_split=0.5, min_samples_leaf=0.5, max_leaf_nodes=10, max_features=0.5, max_depth=2, total=   0.5s\n",
      "[CV] oob_score=False, n_estimators=200, min_samples_split=0.5, min_samples_leaf=0.5, max_leaf_nodes=10, max_features=0.5, max_depth=2 \n",
      "[CV]  oob_score=True, n_estimators=200, min_samples_split=0.5, min_samples_leaf=0.1, max_leaf_nodes=10, max_features=0.5, max_depth=10, total=   0.5s\n",
      "[CV] oob_score=True, n_estimators=200, min_samples_split=0.5, min_samples_leaf=0.1, max_leaf_nodes=10, max_features=0.5, max_depth=10 \n",
      "[CV]  oob_score=False, n_estimators=200, min_samples_split=0.1, min_samples_leaf=0.1, max_leaf_nodes=2, max_features=0.5, max_depth=10, total=   0.5s\n",
      "[CV] oob_score=False, n_estimators=200, min_samples_split=0.1, min_samples_leaf=0.1, max_leaf_nodes=2, max_features=0.5, max_depth=10 \n",
      "[CV]  oob_score=True, n_estimators=200, min_samples_split=0.5, min_samples_leaf=0.1, max_leaf_nodes=100, max_features=0.5, max_depth=20, total=   0.5s\n",
      "[CV] oob_score=True, n_estimators=200, min_samples_split=0.5, min_samples_leaf=0.1, max_leaf_nodes=100, max_features=0.5, max_depth=20 \n",
      "[CV]  oob_score=False, n_estimators=200, min_samples_split=0.1, min_samples_leaf=0.1, max_leaf_nodes=2, max_features=0.5, max_depth=10, total=   0.4s\n",
      "[CV] oob_score=False, n_estimators=200, min_samples_split=0.1, min_samples_leaf=0.1, max_leaf_nodes=2, max_features=0.5, max_depth=10 \n",
      "[CV]  oob_score=False, n_estimators=200, min_samples_split=0.5, min_samples_leaf=0.5, max_leaf_nodes=10, max_features=0.5, max_depth=2, total=   0.5s\n",
      "[CV] oob_score=False, n_estimators=200, min_samples_split=0.5, min_samples_leaf=0.5, max_leaf_nodes=10, max_features=0.5, max_depth=2 \n",
      "[CV]  oob_score=True, n_estimators=200, min_samples_split=0.5, min_samples_leaf=0.1, max_leaf_nodes=10, max_features=0.5, max_depth=10, total=   0.5s\n",
      "[CV] oob_score=True, n_estimators=200, min_samples_split=0.5, min_samples_leaf=0.1, max_leaf_nodes=10, max_features=0.5, max_depth=10 \n",
      "[CV]  oob_score=True, n_estimators=200, min_samples_split=0.5, min_samples_leaf=0.1, max_leaf_nodes=100, max_features=0.5, max_depth=20, total=   0.5s\n",
      "[CV] oob_score=False, n_estimators=10, min_samples_split=0.1, min_samples_leaf=0.1, max_leaf_nodes=10, max_features=0.1, max_depth=10 \n",
      "[CV]  oob_score=False, n_estimators=10, min_samples_split=0.1, min_samples_leaf=0.1, max_leaf_nodes=10, max_features=0.1, max_depth=10, total=   0.0s\n",
      "[CV] oob_score=False, n_estimators=10, min_samples_split=0.1, min_samples_leaf=0.1, max_leaf_nodes=10, max_features=0.1, max_depth=10 \n",
      "[CV]  oob_score=False, n_estimators=200, min_samples_split=0.5, min_samples_leaf=0.5, max_leaf_nodes=10, max_features=0.5, max_depth=2, total=   0.4s\n",
      "[CV] oob_score=False, n_estimators=200, min_samples_split=0.5, min_samples_leaf=0.5, max_leaf_nodes=10, max_features=0.5, max_depth=2 \n",
      "[CV]  oob_score=False, n_estimators=10, min_samples_split=0.1, min_samples_leaf=0.1, max_leaf_nodes=10, max_features=0.1, max_depth=10, total=   0.0s\n",
      "[CV] oob_score=False, n_estimators=10, min_samples_split=0.1, min_samples_leaf=0.1, max_leaf_nodes=10, max_features=0.1, max_depth=10 \n",
      "[CV]  oob_score=False, n_estimators=10, min_samples_split=0.1, min_samples_leaf=0.1, max_leaf_nodes=10, max_features=0.1, max_depth=10, total=   0.0s\n",
      "[CV] oob_score=False, n_estimators=10, min_samples_split=0.1, min_samples_leaf=0.1, max_leaf_nodes=10, max_features=0.1, max_depth=10 \n",
      "[CV]  oob_score=False, n_estimators=10, min_samples_split=0.1, min_samples_leaf=0.1, max_leaf_nodes=10, max_features=0.1, max_depth=10, total=   0.0s\n",
      "[CV]  oob_score=False, n_estimators=200, min_samples_split=0.1, min_samples_leaf=0.1, max_leaf_nodes=2, max_features=0.5, max_depth=10, total=   0.4s\n",
      "[CV] oob_score=True, n_estimators=10, min_samples_split=0.5, min_samples_leaf=0.1, max_leaf_nodes=10, max_features=0.1, max_depth=2 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/isaac/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/home/isaac/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  oob_score=True, n_estimators=10, min_samples_split=0.5, min_samples_leaf=0.1, max_leaf_nodes=10, max_features=0.1, max_depth=2, total=   0.0s\n",
      "[CV] oob_score=True, n_estimators=10, min_samples_split=0.5, min_samples_leaf=0.1, max_leaf_nodes=10, max_features=0.1, max_depth=2 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/isaac/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/home/isaac/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  oob_score=True, n_estimators=10, min_samples_split=0.5, min_samples_leaf=0.1, max_leaf_nodes=10, max_features=0.1, max_depth=2, total=   0.0s\n",
      "[CV] oob_score=True, n_estimators=10, min_samples_split=0.5, min_samples_leaf=0.1, max_leaf_nodes=10, max_features=0.1, max_depth=2 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/isaac/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/home/isaac/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  oob_score=True, n_estimators=10, min_samples_split=0.5, min_samples_leaf=0.1, max_leaf_nodes=10, max_features=0.1, max_depth=2, total=   0.0s\n",
      "[CV] oob_score=True, n_estimators=10, min_samples_split=0.5, min_samples_leaf=0.1, max_leaf_nodes=10, max_features=0.1, max_depth=2 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/isaac/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/home/isaac/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  oob_score=True, n_estimators=10, min_samples_split=0.5, min_samples_leaf=0.1, max_leaf_nodes=10, max_features=0.1, max_depth=2, total=   0.0s\n",
      "[CV]  oob_score=True, n_estimators=200, min_samples_split=0.5, min_samples_leaf=0.1, max_leaf_nodes=10, max_features=0.5, max_depth=10, total=   0.6s\n",
      "[CV] oob_score=False, n_estimators=200, min_samples_split=0.5, min_samples_leaf=0.1, max_leaf_nodes=10, max_features=0.5, max_depth=20 \n",
      "[CV]  oob_score=False, n_estimators=200, min_samples_split=0.5, min_samples_leaf=0.5, max_leaf_nodes=10, max_features=0.5, max_depth=2, total=   0.4s\n",
      "[CV]  oob_score=False, n_estimators=200, min_samples_split=0.5, min_samples_leaf=0.1, max_leaf_nodes=10, max_features=0.5, max_depth=20, total=   0.3s\n",
      "[CV] oob_score=False, n_estimators=200, min_samples_split=0.5, min_samples_leaf=0.1, max_leaf_nodes=10, max_features=0.5, max_depth=20 \n",
      "[CV]  oob_score=False, n_estimators=200, min_samples_split=0.5, min_samples_leaf=0.1, max_leaf_nodes=10, max_features=0.5, max_depth=20, total=   0.2s\n",
      "[CV] oob_score=False, n_estimators=200, min_samples_split=0.5, min_samples_leaf=0.1, max_leaf_nodes=10, max_features=0.5, max_depth=20 \n",
      "[CV]  oob_score=False, n_estimators=200, min_samples_split=0.5, min_samples_leaf=0.1, max_leaf_nodes=10, max_features=0.5, max_depth=20, total=   0.2s\n",
      "[CV] oob_score=False, n_estimators=200, min_samples_split=0.5, min_samples_leaf=0.1, max_leaf_nodes=10, max_features=0.5, max_depth=20 \n",
      "[CV]  oob_score=False, n_estimators=200, min_samples_split=0.5, min_samples_leaf=0.1, max_leaf_nodes=10, max_features=0.5, max_depth=20, total=   0.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:    3.9s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=4, error_score='raise',\n",
       "          estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=42, verbose=0, warm_start=False),\n",
       "          fit_params=None, iid=True, n_iter=10, n_jobs=-1,\n",
       "          param_distributions={'n_estimators': [10, 200], 'max_features': [0.1, 0.5], 'max_depth': [2, 10, 20], 'min_samples_split': [0.1, 0.5], 'oob_score': [True, False], 'min_samples_leaf': [0.1, 0.5], 'max_leaf_nodes': [2, 10, 100]},\n",
       "          pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "          return_train_score='warn', scoring='roc_auc', verbose=2)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest_class = RandomForestClassifier(random_state = 42)\n",
    "\n",
    "n_estimators = [10, 200]\n",
    "max_features = [0.1, 0.5]\n",
    "max_depth = [2, 10, 20] \n",
    "oob_score = [True, False]\n",
    "min_samples_split = [0.1, 0.5]\n",
    "min_samples_leaf = [0.1, 0.5] \n",
    "max_leaf_nodes = [2, 10, 100]\n",
    "\n",
    "param_grid_forest = {'n_estimators' : n_estimators, 'max_features' : max_features,\n",
    "                     'max_depth' : max_depth, 'min_samples_split' : min_samples_split,\n",
    "                    'oob_score' : oob_score, 'min_samples_leaf': min_samples_leaf, \n",
    "                     'max_leaf_nodes' : max_leaf_nodes}\n",
    "\n",
    "\n",
    "rand_search_forest = RandomizedSearchCV(forest_class, param_grid_forest, cv = 4, scoring='roc_auc', refit = True,\n",
    "                                 n_jobs = -1, verbose=2)\n",
    "\n",
    "rand_search_forest.fit(X_train, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'oob_score': True, 'n_estimators': 200, 'min_samples_split': 0.5, 'min_samples_leaf': 0.1, 'max_leaf_nodes': 100, 'max_features': 0.5, 'max_depth': 20}\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=20, max_features=0.5, max_leaf_nodes=100,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=0.1, min_samples_split=0.5,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=200, n_jobs=1,\n",
      "            oob_score=True, random_state=42, verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "print(rand_search_forest.best_params_)\n",
    "\n",
    "print(rand_search_forest.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7867564534231201"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand_forest_randomized = rand_search_forest.best_estimator_\n",
    "\n",
    "y_pred_rand_forest_randomized = rand_forest_randomized.predict(X_train)\n",
    "accuracy_score(train_labels, y_pred_rand_forest_randomized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[468  81]\n",
      " [109 233]]\n",
      "Precision:  0.7420382165605095\n",
      "Recall:  0.6812865497076024\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(train_labels, y_pred_rand_forest_randomized))\n",
    "\n",
    "print(\"Precision: \", precision_score(train_labels, y_pred_rand_forest_randomized))\n",
    "\n",
    "print(\"Recall: \", recall_score(train_labels, y_pred_rand_forest_randomized))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AdaBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 10 candidates, totalling 40 fits\n",
      "[CV] n_estimators=200, learning_rate=0.1, algorithm=SAMME ............\n",
      "[CV] n_estimators=200, learning_rate=0.1, algorithm=SAMME ............\n",
      "[CV] n_estimators=200, learning_rate=0.1, algorithm=SAMME ............\n",
      "[CV] n_estimators=200, learning_rate=0.1, algorithm=SAMME ............\n",
      "[CV]  n_estimators=200, learning_rate=0.1, algorithm=SAMME, total=   0.4s\n",
      "[CV] n_estimators=200, learning_rate=0.5, algorithm=SAMME ............\n",
      "[CV]  n_estimators=200, learning_rate=0.1, algorithm=SAMME, total=   0.5s\n",
      "[CV] n_estimators=200, learning_rate=0.5, algorithm=SAMME ............\n",
      "[CV]  n_estimators=200, learning_rate=0.1, algorithm=SAMME, total=   0.5s\n",
      "[CV]  n_estimators=200, learning_rate=0.1, algorithm=SAMME, total=   0.5s\n",
      "[CV] n_estimators=200, learning_rate=0.5, algorithm=SAMME ............\n",
      "[CV] n_estimators=200, learning_rate=0.5, algorithm=SAMME ............\n",
      "[CV]  n_estimators=200, learning_rate=0.5, algorithm=SAMME, total=   0.4s\n",
      "[CV] n_estimators=100, learning_rate=0.9, algorithm=SAMME.R ..........\n",
      "[CV]  n_estimators=200, learning_rate=0.5, algorithm=SAMME, total=   0.4s\n",
      "[CV] n_estimators=100, learning_rate=0.9, algorithm=SAMME.R ..........\n",
      "[CV]  n_estimators=200, learning_rate=0.5, algorithm=SAMME, total=   0.4s\n",
      "[CV] n_estimators=100, learning_rate=0.9, algorithm=SAMME.R ..........\n",
      "[CV]  n_estimators=200, learning_rate=0.5, algorithm=SAMME, total=   0.4s\n",
      "[CV] n_estimators=100, learning_rate=0.9, algorithm=SAMME.R ..........\n",
      "[CV]  n_estimators=100, learning_rate=0.9, algorithm=SAMME.R, total=   0.2s\n",
      "[CV] n_estimators=3, learning_rate=0.5, algorithm=SAMME.R ............\n",
      "[CV]  n_estimators=3, learning_rate=0.5, algorithm=SAMME.R, total=   0.0s\n",
      "[CV] n_estimators=3, learning_rate=0.5, algorithm=SAMME.R ............\n",
      "[CV]  n_estimators=3, learning_rate=0.5, algorithm=SAMME.R, total=   0.0s\n",
      "[CV] n_estimators=3, learning_rate=0.5, algorithm=SAMME.R ............\n",
      "[CV]  n_estimators=3, learning_rate=0.5, algorithm=SAMME.R, total=   0.0s\n",
      "[CV]  n_estimators=100, learning_rate=0.9, algorithm=SAMME.R, total=   0.3s\n",
      "[CV] n_estimators=3, learning_rate=0.5, algorithm=SAMME.R ............\n",
      "[CV] n_estimators=50, learning_rate=0.5, algorithm=SAMME.R ...........\n",
      "[CV]  n_estimators=100, learning_rate=0.9, algorithm=SAMME.R, total=   0.3s\n",
      "[CV]  n_estimators=100, learning_rate=0.9, algorithm=SAMME.R, total=   0.3s\n",
      "[CV]  n_estimators=3, learning_rate=0.5, algorithm=SAMME.R, total=   0.0s\n",
      "[CV] n_estimators=50, learning_rate=0.5, algorithm=SAMME.R ...........\n",
      "[CV] n_estimators=50, learning_rate=0.5, algorithm=SAMME.R ...........\n",
      "[CV] n_estimators=50, learning_rate=0.5, algorithm=SAMME.R ...........\n",
      "[CV]  n_estimators=50, learning_rate=0.5, algorithm=SAMME.R, total=   0.1s\n",
      "[CV] n_estimators=200, learning_rate=0.5, algorithm=SAMME.R ..........\n",
      "[CV]  n_estimators=50, learning_rate=0.5, algorithm=SAMME.R, total=   0.2s\n",
      "[CV] n_estimators=200, learning_rate=0.5, algorithm=SAMME.R ..........\n",
      "[CV]  n_estimators=50, learning_rate=0.5, algorithm=SAMME.R, total=   0.2s\n",
      "[CV] n_estimators=200, learning_rate=0.5, algorithm=SAMME.R ..........\n",
      "[CV]  n_estimators=50, learning_rate=0.5, algorithm=SAMME.R, total=   0.2s\n",
      "[CV] n_estimators=200, learning_rate=0.5, algorithm=SAMME.R ..........\n",
      "[CV]  n_estimators=200, learning_rate=0.5, algorithm=SAMME.R, total=   0.5s\n",
      "[CV] n_estimators=50, learning_rate=0.9, algorithm=SAMME .............\n",
      "[CV]  n_estimators=200, learning_rate=0.5, algorithm=SAMME.R, total=   0.5s\n",
      "[CV]  n_estimators=50, learning_rate=0.9, algorithm=SAMME, total=   0.1s\n",
      "[CV] n_estimators=50, learning_rate=0.9, algorithm=SAMME .............\n",
      "[CV] n_estimators=50, learning_rate=0.9, algorithm=SAMME .............\n",
      "[CV]  n_estimators=200, learning_rate=0.5, algorithm=SAMME.R, total=   0.6s\n",
      "[CV] n_estimators=50, learning_rate=0.9, algorithm=SAMME .............\n",
      "[CV]  n_estimators=200, learning_rate=0.5, algorithm=SAMME.R, total=   0.6s\n",
      "[CV]  n_estimators=50, learning_rate=0.9, algorithm=SAMME, total=   0.1s\n",
      "[CV] n_estimators=3, learning_rate=0.9, algorithm=SAMME.R ............\n",
      "[CV] n_estimators=3, learning_rate=0.9, algorithm=SAMME.R ............\n",
      "[CV]  n_estimators=3, learning_rate=0.9, algorithm=SAMME.R, total=   0.0s\n",
      "[CV] n_estimators=3, learning_rate=0.9, algorithm=SAMME.R ............\n",
      "[CV]  n_estimators=3, learning_rate=0.9, algorithm=SAMME.R, total=   0.0s\n",
      "[CV]  n_estimators=50, learning_rate=0.9, algorithm=SAMME, total=   0.2s\n",
      "[CV] n_estimators=3, learning_rate=0.9, algorithm=SAMME.R ............\n",
      "[CV] n_estimators=50, learning_rate=0.5, algorithm=SAMME .............\n",
      "[CV]  n_estimators=3, learning_rate=0.9, algorithm=SAMME.R, total=   0.0s\n",
      "[CV] n_estimators=50, learning_rate=0.5, algorithm=SAMME .............\n",
      "[CV]  n_estimators=3, learning_rate=0.9, algorithm=SAMME.R, total=   0.0s\n",
      "[CV] n_estimators=50, learning_rate=0.5, algorithm=SAMME .............\n",
      "[CV]  n_estimators=50, learning_rate=0.9, algorithm=SAMME, total=   0.1s\n",
      "[CV] n_estimators=50, learning_rate=0.5, algorithm=SAMME .............\n",
      "[CV]  n_estimators=50, learning_rate=0.5, algorithm=SAMME, total=   0.1s\n",
      "[CV]  n_estimators=50, learning_rate=0.5, algorithm=SAMME, total=   0.1s\n",
      "[CV] n_estimators=20, learning_rate=0.1, algorithm=SAMME.R ...........\n",
      "[CV] n_estimators=20, learning_rate=0.1, algorithm=SAMME.R ...........\n",
      "[CV]  n_estimators=50, learning_rate=0.5, algorithm=SAMME, total=   0.2s\n",
      "[CV] n_estimators=20, learning_rate=0.1, algorithm=SAMME.R ...........\n",
      "[CV]  n_estimators=20, learning_rate=0.1, algorithm=SAMME.R, total=   0.1s\n",
      "[CV] n_estimators=20, learning_rate=0.1, algorithm=SAMME.R ...........\n",
      "[CV]  n_estimators=20, learning_rate=0.1, algorithm=SAMME.R, total=   0.0s\n",
      "[CV]  n_estimators=20, learning_rate=0.1, algorithm=SAMME.R, total=   0.1s\n",
      "[CV]  n_estimators=50, learning_rate=0.5, algorithm=SAMME, total=   0.2s\n",
      "[CV]  n_estimators=20, learning_rate=0.1, algorithm=SAMME.R, total=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:    2.5s\n",
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:    2.6s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=4, error_score='raise',\n",
       "          estimator=AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
       "          learning_rate=1.0, n_estimators=50, random_state=42),\n",
       "          fit_params=None, iid=True, n_iter=10, n_jobs=-1,\n",
       "          param_distributions={'n_estimators': [3, 20, 50, 100, 200], 'learning_rate': [0.1, 0.5, 0.9], 'algorithm': ['SAMME', 'SAMME.R']},\n",
       "          pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "          return_train_score='warn', scoring='roc_auc', verbose=2)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ada_class = AdaBoostClassifier(random_state = 42)\n",
    "\n",
    "n_estimators = [3, 20, 50, 100, 200]\n",
    "learning_rate = [0.1, 0.5, 0.9]\n",
    "algorithm = ['SAMME', 'SAMME.R']\n",
    "\n",
    "param_grid_ada = {'n_estimators' : n_estimators, 'learning_rate' : learning_rate, 'algorithm' : algorithm}\n",
    "\n",
    "rand_search_ada = RandomizedSearchCV(ada_class, param_grid_ada, cv = 4, scoring='roc_auc', refit = True, n_jobs = -1, verbose = 2)\n",
    "\n",
    "rand_search_ada.fit(X_train, train_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 50, 'learning_rate': 0.5, 'algorithm': 'SAMME.R'}\n",
      "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
      "          learning_rate=0.5, n_estimators=50, random_state=42)\n"
     ]
    }
   ],
   "source": [
    "print(rand_search_ada.best_params_)\n",
    "\n",
    "print(rand_search_ada.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8226711560044894"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ada_randomized = rand_search_ada.best_estimator_\n",
    "\n",
    "y_pred_ada_randomized = ada_randomized.predict(X_train)\n",
    "accuracy_score(train_labels, y_pred_ada_randomized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[474  75]\n",
      " [ 83 259]]\n",
      "Precision:  0.7754491017964071\n",
      "Recall:  0.7573099415204678\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(train_labels, y_pred_ada_randomized))\n",
    "\n",
    "print(\"Precision: \", precision_score(train_labels, y_pred_ada_randomized))\n",
    "\n",
    "print(\"Recall: \", recall_score(train_labels, y_pred_ada_randomized))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, AdaBoost so far has the best Precision and Recall. Nice, nice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra Trees Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 10 candidates, totalling 40 fits\n",
      "[CV] n_estimators=50, min_samples_split=0.1, min_samples_leaf=0.5, max_features=0.1, max_depth=100 \n",
      "[CV] n_estimators=50, min_samples_split=0.1, min_samples_leaf=0.5, max_features=0.1, max_depth=100 \n",
      "[CV] n_estimators=50, min_samples_split=0.1, min_samples_leaf=0.5, max_features=0.1, max_depth=100 \n",
      "[CV] n_estimators=50, min_samples_split=0.1, min_samples_leaf=0.5, max_features=0.1, max_depth=100 \n",
      "[CV]  n_estimators=50, min_samples_split=0.1, min_samples_leaf=0.5, max_features=0.1, max_depth=100, total=   0.1s\n",
      "[CV] n_estimators=50, min_samples_split=0.1, min_samples_leaf=0.1, max_features=0.5, max_depth=100 \n",
      "[CV]  n_estimators=50, min_samples_split=0.1, min_samples_leaf=0.5, max_features=0.1, max_depth=100, total=   0.1s\n",
      "[CV]  n_estimators=50, min_samples_split=0.1, min_samples_leaf=0.5, max_features=0.1, max_depth=100, total=   0.1s\n",
      "[CV] n_estimators=50, min_samples_split=0.1, min_samples_leaf=0.1, max_features=0.5, max_depth=100 \n",
      "[CV] n_estimators=50, min_samples_split=0.1, min_samples_leaf=0.1, max_features=0.5, max_depth=100 \n",
      "[CV]  n_estimators=50, min_samples_split=0.1, min_samples_leaf=0.5, max_features=0.1, max_depth=100, total=   0.1s\n",
      "[CV] n_estimators=50, min_samples_split=0.1, min_samples_leaf=0.1, max_features=0.5, max_depth=100 \n",
      "[CV]  n_estimators=50, min_samples_split=0.1, min_samples_leaf=0.1, max_features=0.5, max_depth=100, total=   0.1s\n",
      "[CV] n_estimators=200, min_samples_split=0.1, min_samples_leaf=0.1, max_features=0.5, max_depth=50 \n",
      "[CV]  n_estimators=50, min_samples_split=0.1, min_samples_leaf=0.1, max_features=0.5, max_depth=100, total=   0.1s\n",
      "[CV] n_estimators=200, min_samples_split=0.1, min_samples_leaf=0.1, max_features=0.5, max_depth=50 \n",
      "[CV]  n_estimators=50, min_samples_split=0.1, min_samples_leaf=0.1, max_features=0.5, max_depth=100, total=   0.2s\n",
      "[CV] n_estimators=100, min_samples_split=0.5, min_samples_leaf=0.1, max_features=0.1, max_depth=2 \n",
      "[CV]  n_estimators=50, min_samples_split=0.1, min_samples_leaf=0.1, max_features=0.5, max_depth=100, total=   0.2s\n",
      "[CV] n_estimators=100, min_samples_split=0.5, min_samples_leaf=0.1, max_features=0.1, max_depth=2 \n",
      "[CV]  n_estimators=100, min_samples_split=0.5, min_samples_leaf=0.1, max_features=0.1, max_depth=2, total=   0.2s\n",
      "[CV] n_estimators=100, min_samples_split=0.5, min_samples_leaf=0.1, max_features=0.1, max_depth=2 \n",
      "[CV]  n_estimators=100, min_samples_split=0.5, min_samples_leaf=0.1, max_features=0.1, max_depth=2, total=   0.3s\n",
      "[CV] n_estimators=100, min_samples_split=0.5, min_samples_leaf=0.1, max_features=0.1, max_depth=2 \n",
      "[CV]  n_estimators=200, min_samples_split=0.1, min_samples_leaf=0.1, max_features=0.5, max_depth=50, total=   0.4s\n",
      "[CV] n_estimators=200, min_samples_split=0.1, min_samples_leaf=0.1, max_features=0.5, max_depth=50 \n",
      "[CV]  n_estimators=200, min_samples_split=0.1, min_samples_leaf=0.1, max_features=0.5, max_depth=50, total=   0.5s\n",
      "[CV]  n_estimators=100, min_samples_split=0.5, min_samples_leaf=0.1, max_features=0.1, max_depth=2, total=   0.2s\n",
      "[CV] n_estimators=100, min_samples_split=0.5, min_samples_leaf=0.5, max_features=0.5, max_depth=50 \n",
      "[CV] n_estimators=200, min_samples_split=0.1, min_samples_leaf=0.1, max_features=0.5, max_depth=50 \n",
      "[CV]  n_estimators=100, min_samples_split=0.5, min_samples_leaf=0.1, max_features=0.1, max_depth=2, total=   0.3s\n",
      "[CV] n_estimators=100, min_samples_split=0.5, min_samples_leaf=0.5, max_features=0.5, max_depth=50 \n",
      "[CV]  n_estimators=100, min_samples_split=0.5, min_samples_leaf=0.5, max_features=0.5, max_depth=50, total=   0.2s\n",
      "[CV] n_estimators=100, min_samples_split=0.5, min_samples_leaf=0.5, max_features=0.5, max_depth=50 \n",
      "[CV]  n_estimators=100, min_samples_split=0.5, min_samples_leaf=0.5, max_features=0.5, max_depth=50, total=   0.3s\n",
      "[CV] n_estimators=100, min_samples_split=0.5, min_samples_leaf=0.5, max_features=0.5, max_depth=50 \n",
      "[CV]  n_estimators=200, min_samples_split=0.1, min_samples_leaf=0.1, max_features=0.5, max_depth=50, total=   0.5s\n",
      "[CV] n_estimators=200, min_samples_split=0.1, min_samples_leaf=0.1, max_features=0.1, max_depth=50 \n",
      "[CV]  n_estimators=100, min_samples_split=0.5, min_samples_leaf=0.5, max_features=0.5, max_depth=50, total=   0.2s\n",
      "[CV] n_estimators=200, min_samples_split=0.1, min_samples_leaf=0.1, max_features=0.1, max_depth=50 \n",
      "[CV]  n_estimators=200, min_samples_split=0.1, min_samples_leaf=0.1, max_features=0.5, max_depth=50, total=   0.4s\n",
      "[CV] n_estimators=100, min_samples_split=0.1, min_samples_leaf=0.5, max_features=0.5, max_depth=100 \n",
      "[CV]  n_estimators=100, min_samples_split=0.5, min_samples_leaf=0.5, max_features=0.5, max_depth=50, total=   0.2s\n",
      "[CV] n_estimators=100, min_samples_split=0.1, min_samples_leaf=0.5, max_features=0.5, max_depth=100 \n",
      "[CV]  n_estimators=100, min_samples_split=0.1, min_samples_leaf=0.5, max_features=0.5, max_depth=100, total=   0.2s\n",
      "[CV] n_estimators=100, min_samples_split=0.1, min_samples_leaf=0.5, max_features=0.5, max_depth=100 \n",
      "[CV]  n_estimators=100, min_samples_split=0.1, min_samples_leaf=0.5, max_features=0.5, max_depth=100, total=   0.2s\n",
      "[CV] n_estimators=100, min_samples_split=0.1, min_samples_leaf=0.5, max_features=0.5, max_depth=100 \n",
      "[CV]  n_estimators=200, min_samples_split=0.1, min_samples_leaf=0.1, max_features=0.1, max_depth=50, total=   0.5s\n",
      "[CV] n_estimators=200, min_samples_split=0.1, min_samples_leaf=0.1, max_features=0.1, max_depth=50 \n",
      "[CV]  n_estimators=100, min_samples_split=0.1, min_samples_leaf=0.5, max_features=0.5, max_depth=100, total=   0.2s\n",
      "[CV] n_estimators=50, min_samples_split=0.1, min_samples_leaf=0.1, max_features=0.5, max_depth=50 \n",
      "[CV]  n_estimators=200, min_samples_split=0.1, min_samples_leaf=0.1, max_features=0.1, max_depth=50, total=   0.5s\n",
      "[CV] n_estimators=200, min_samples_split=0.1, min_samples_leaf=0.1, max_features=0.1, max_depth=50 \n",
      "[CV]  n_estimators=100, min_samples_split=0.1, min_samples_leaf=0.5, max_features=0.5, max_depth=100, total=   0.2s\n",
      "[CV] n_estimators=50, min_samples_split=0.1, min_samples_leaf=0.1, max_features=0.5, max_depth=50 \n",
      "[CV]  n_estimators=50, min_samples_split=0.1, min_samples_leaf=0.1, max_features=0.5, max_depth=50, total=   0.1s\n",
      "[CV] n_estimators=50, min_samples_split=0.1, min_samples_leaf=0.1, max_features=0.5, max_depth=50 \n",
      "[CV]  n_estimators=50, min_samples_split=0.1, min_samples_leaf=0.1, max_features=0.5, max_depth=50, total=   0.1s\n",
      "[CV] n_estimators=200, min_samples_split=0.5, min_samples_leaf=0.1, max_features=0.1, max_depth=100 \n",
      "[CV]  n_estimators=50, min_samples_split=0.1, min_samples_leaf=0.1, max_features=0.5, max_depth=50, total=   0.2s\n",
      "[CV] n_estimators=50, min_samples_split=0.1, min_samples_leaf=0.1, max_features=0.5, max_depth=50 \n",
      "[CV]  n_estimators=200, min_samples_split=0.1, min_samples_leaf=0.1, max_features=0.1, max_depth=50, total=   0.4s\n",
      "[CV] n_estimators=200, min_samples_split=0.5, min_samples_leaf=0.1, max_features=0.1, max_depth=100 \n",
      "[CV]  n_estimators=50, min_samples_split=0.1, min_samples_leaf=0.1, max_features=0.5, max_depth=50, total=   0.1s\n",
      "[CV] n_estimators=100, min_samples_split=0.1, min_samples_leaf=0.1, max_features=0.1, max_depth=100 \n",
      "[CV]  n_estimators=200, min_samples_split=0.1, min_samples_leaf=0.1, max_features=0.1, max_depth=50, total=   0.6s\n",
      "[CV] n_estimators=100, min_samples_split=0.1, min_samples_leaf=0.1, max_features=0.1, max_depth=100 \n",
      "[CV]  n_estimators=100, min_samples_split=0.1, min_samples_leaf=0.1, max_features=0.1, max_depth=100, total=   0.2s\n",
      "[CV] n_estimators=100, min_samples_split=0.1, min_samples_leaf=0.1, max_features=0.1, max_depth=100 \n",
      "[CV]  n_estimators=200, min_samples_split=0.5, min_samples_leaf=0.1, max_features=0.1, max_depth=100, total=   0.5s\n",
      "[CV] n_estimators=200, min_samples_split=0.5, min_samples_leaf=0.1, max_features=0.1, max_depth=100 \n",
      "[CV]  n_estimators=200, min_samples_split=0.5, min_samples_leaf=0.1, max_features=0.1, max_depth=100, total=   0.4s\n",
      "[CV] n_estimators=200, min_samples_split=0.5, min_samples_leaf=0.1, max_features=0.1, max_depth=100 \n",
      "[CV]  n_estimators=100, min_samples_split=0.1, min_samples_leaf=0.1, max_features=0.1, max_depth=100, total=   0.2s\n",
      "[CV]  n_estimators=100, min_samples_split=0.1, min_samples_leaf=0.1, max_features=0.1, max_depth=100, total=   0.3s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] n_estimators=100, min_samples_split=0.1, min_samples_leaf=0.1, max_features=0.1, max_depth=100 \n",
      "[CV]  n_estimators=100, min_samples_split=0.1, min_samples_leaf=0.1, max_features=0.1, max_depth=100, total=   0.2s\n",
      "[CV]  n_estimators=200, min_samples_split=0.5, min_samples_leaf=0.1, max_features=0.1, max_depth=100, total=   0.3s\n",
      "[CV]  n_estimators=200, min_samples_split=0.5, min_samples_leaf=0.1, max_features=0.1, max_depth=100, total=   0.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:    3.0s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=4, error_score='raise',\n",
       "          estimator=ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
       "           max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "           oob_score=False, random_state=42, verbose=0, warm_start=False),\n",
       "          fit_params=None, iid=True, n_iter=10, n_jobs=-1,\n",
       "          param_distributions={'n_estimators': [3, 50, 100, 200], 'max_features': [0.1, 0.5], 'max_depth': [2, 50, 100], 'min_samples_split': [0.1, 0.5], 'min_samples_leaf': [0.1, 0.5]},\n",
       "          pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "          return_train_score='warn', scoring='roc_auc', verbose=2)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extra_tree_class = ExtraTreesClassifier(random_state = 42)\n",
    "\n",
    "n_estimators = [3, 50, 100, 200]\n",
    "max_features = [0.1, 0.5]\n",
    "max_depth = [2, 50, 100]\n",
    "min_samples_split = [0.1, 0.5]\n",
    "min_samples_leaf = [0.1, 0.5] # Mhm, this one leads to accuracy of test and train sets being the same.\n",
    "\n",
    "#param_grid_extra_trees = {'n_estimators' : n_estimators, 'max_features' : max_features,\n",
    "#                          'min_samples_leaf' : min_samples_leaf}\n",
    "param_grid_extra_trees = {'n_estimators' : n_estimators, 'max_features' : max_features,\n",
    "                         'max_depth' : max_depth, 'min_samples_split' : min_samples_split,\n",
    "                         'min_samples_leaf' : min_samples_leaf}\n",
    "\n",
    "\n",
    "rand_search_extra_trees = RandomizedSearchCV(extra_tree_class, param_grid_extra_trees, cv = 4, scoring='roc_auc', \n",
    "                               refit = True, n_jobs = -1, verbose = 2)\n",
    "\n",
    "rand_search_extra_trees.fit(X_train, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 200, 'min_samples_split': 0.1, 'min_samples_leaf': 0.1, 'max_features': 0.5, 'max_depth': 50}\n",
      "ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
      "           max_depth=50, max_features=0.5, max_leaf_nodes=None,\n",
      "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "           min_samples_leaf=0.1, min_samples_split=0.1,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=200, n_jobs=1,\n",
      "           oob_score=False, random_state=42, verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "print(rand_search_extra_trees.best_params_)\n",
    "\n",
    "print(rand_search_extra_trees.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7867564534231201"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extra_trees_randomized = rand_search_extra_trees.best_estimator_\n",
    "\n",
    "y_pred_extra_trees_randomized = extra_trees_randomized.predict(X_train)\n",
    "accuracy_score(train_labels, y_pred_extra_trees_randomized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[468  81]\n",
      " [109 233]]\n",
      "Precision:  0.7420382165605095\n",
      "Recall:  0.6812865497076024\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(train_labels, y_pred_extra_trees_randomized))\n",
    "\n",
    "print(\"Precision: \", precision_score(train_labels, y_pred_extra_trees_randomized))\n",
    "\n",
    "print(\"Recall: \", recall_score(train_labels, y_pred_extra_trees_randomized))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 10 candidates, totalling 40 fits\n",
      "[CV] n_estimators=3, min_samples_split=0.1, min_samples_leaf=0.1, max_leaf_nodes=2, max_features=0.1, max_depth=100, learning_rate=0.5 \n",
      "[CV] n_estimators=3, min_samples_split=0.1, min_samples_leaf=0.1, max_leaf_nodes=2, max_features=0.1, max_depth=100, learning_rate=0.5 \n",
      "[CV] n_estimators=3, min_samples_split=0.1, min_samples_leaf=0.1, max_leaf_nodes=2, max_features=0.1, max_depth=100, learning_rate=0.5 \n",
      "[CV]  n_estimators=3, min_samples_split=0.1, min_samples_leaf=0.1, max_leaf_nodes=2, max_features=0.1, max_depth=100, learning_rate=0.5, total=   0.0s\n",
      "[CV]  n_estimators=3, min_samples_split=0.1, min_samples_leaf=0.1, max_leaf_nodes=2, max_features=0.1, max_depth=100, learning_rate=0.5, total=   0.0s\n",
      "[CV]  n_estimators=3, min_samples_split=0.1, min_samples_leaf=0.1, max_leaf_nodes=2, max_features=0.1, max_depth=100, learning_rate=0.5, total=   0.0s\n",
      "[CV] n_estimators=3, min_samples_split=0.5, min_samples_leaf=0.5, max_leaf_nodes=50, max_features=0.1, max_depth=50, learning_rate=0.1 \n",
      "[CV] n_estimators=3, min_samples_split=0.1, min_samples_leaf=0.1, max_leaf_nodes=2, max_features=0.1, max_depth=100, learning_rate=0.5 \n",
      "[CV]  n_estimators=3, min_samples_split=0.5, min_samples_leaf=0.5, max_leaf_nodes=50, max_features=0.1, max_depth=50, learning_rate=0.1, total=   0.0s\n",
      "[CV]  n_estimators=3, min_samples_split=0.5, min_samples_leaf=0.5, max_leaf_nodes=50, max_features=0.1, max_depth=50, learning_rate=0.1, total=   0.0s\n",
      "[CV]  n_estimators=3, min_samples_split=0.1, min_samples_leaf=0.1, max_leaf_nodes=2, max_features=0.1, max_depth=100, learning_rate=0.5, total=   0.0s\n",
      "[CV] n_estimators=3, min_samples_split=0.5, min_samples_leaf=0.5, max_leaf_nodes=50, max_features=0.1, max_depth=50, learning_rate=0.1 \n",
      "[CV] n_estimators=200, min_samples_split=0.5, min_samples_leaf=0.1, max_leaf_nodes=100, max_features=0.5, max_depth=50, learning_rate=0.1 \n",
      "[CV] n_estimators=3, min_samples_split=0.5, min_samples_leaf=0.5, max_leaf_nodes=50, max_features=0.1, max_depth=50, learning_rate=0.1 \n",
      "[CV] n_estimators=3, min_samples_split=0.5, min_samples_leaf=0.5, max_leaf_nodes=2, max_features=0.5, max_depth=100, learning_rate=0.5 \n",
      "[CV] n_estimators=3, min_samples_split=0.5, min_samples_leaf=0.5, max_leaf_nodes=50, max_features=0.1, max_depth=50, learning_rate=0.1 \n",
      "[CV]  n_estimators=3, min_samples_split=0.5, min_samples_leaf=0.5, max_leaf_nodes=50, max_features=0.1, max_depth=50, learning_rate=0.1, total=   0.0s\n",
      "[CV]  n_estimators=3, min_samples_split=0.5, min_samples_leaf=0.5, max_leaf_nodes=50, max_features=0.1, max_depth=50, learning_rate=0.1, total=   0.0s\n",
      "[CV]  n_estimators=3, min_samples_split=0.5, min_samples_leaf=0.5, max_leaf_nodes=2, max_features=0.5, max_depth=100, learning_rate=0.5, total=   0.0s\n",
      "[CV] n_estimators=3, min_samples_split=0.5, min_samples_leaf=0.5, max_leaf_nodes=2, max_features=0.5, max_depth=100, learning_rate=0.5 \n",
      "[CV]  n_estimators=3, min_samples_split=0.5, min_samples_leaf=0.5, max_leaf_nodes=2, max_features=0.5, max_depth=100, learning_rate=0.5, total=   0.0s\n",
      "[CV] n_estimators=3, min_samples_split=0.5, min_samples_leaf=0.5, max_leaf_nodes=2, max_features=0.5, max_depth=100, learning_rate=0.5 \n",
      "[CV]  n_estimators=3, min_samples_split=0.5, min_samples_leaf=0.5, max_leaf_nodes=2, max_features=0.5, max_depth=100, learning_rate=0.5, total=   0.0s\n",
      "[CV] n_estimators=3, min_samples_split=0.5, min_samples_leaf=0.5, max_leaf_nodes=2, max_features=0.5, max_depth=100, learning_rate=0.5 \n",
      "[CV]  n_estimators=3, min_samples_split=0.5, min_samples_leaf=0.5, max_leaf_nodes=2, max_features=0.5, max_depth=100, learning_rate=0.5, total=   0.0s\n",
      "[CV] n_estimators=200, min_samples_split=0.1, min_samples_leaf=0.1, max_leaf_nodes=2, max_features=0.1, max_depth=100, learning_rate=0.1 \n",
      "[CV]  n_estimators=200, min_samples_split=0.5, min_samples_leaf=0.1, max_leaf_nodes=100, max_features=0.5, max_depth=50, learning_rate=0.1, total=   0.1s\n",
      "[CV] n_estimators=200, min_samples_split=0.5, min_samples_leaf=0.1, max_leaf_nodes=100, max_features=0.5, max_depth=50, learning_rate=0.1 \n",
      "[CV]  n_estimators=200, min_samples_split=0.1, min_samples_leaf=0.1, max_leaf_nodes=2, max_features=0.1, max_depth=100, learning_rate=0.1, total=   0.1s\n",
      "[CV] n_estimators=200, min_samples_split=0.1, min_samples_leaf=0.1, max_leaf_nodes=2, max_features=0.1, max_depth=100, learning_rate=0.1 \n",
      "[CV]  n_estimators=200, min_samples_split=0.5, min_samples_leaf=0.1, max_leaf_nodes=100, max_features=0.5, max_depth=50, learning_rate=0.1, total=   0.1s\n",
      "[CV] n_estimators=200, min_samples_split=0.5, min_samples_leaf=0.1, max_leaf_nodes=100, max_features=0.5, max_depth=50, learning_rate=0.1 \n",
      "[CV]  n_estimators=200, min_samples_split=0.1, min_samples_leaf=0.1, max_leaf_nodes=2, max_features=0.1, max_depth=100, learning_rate=0.1, total=   0.1s\n",
      "[CV] n_estimators=200, min_samples_split=0.1, min_samples_leaf=0.1, max_leaf_nodes=2, max_features=0.1, max_depth=100, learning_rate=0.1 \n",
      "[CV]  n_estimators=200, min_samples_split=0.1, min_samples_leaf=0.1, max_leaf_nodes=2, max_features=0.1, max_depth=100, learning_rate=0.1, total=   0.1s\n",
      "[CV]  n_estimators=200, min_samples_split=0.5, min_samples_leaf=0.1, max_leaf_nodes=100, max_features=0.5, max_depth=50, learning_rate=0.1, total=   0.1s\n",
      "[CV] n_estimators=200, min_samples_split=0.1, min_samples_leaf=0.1, max_leaf_nodes=2, max_features=0.1, max_depth=100, learning_rate=0.1 \n",
      "[CV] n_estimators=200, min_samples_split=0.5, min_samples_leaf=0.1, max_leaf_nodes=100, max_features=0.5, max_depth=50, learning_rate=0.1 \n",
      "[CV]  n_estimators=200, min_samples_split=0.1, min_samples_leaf=0.1, max_leaf_nodes=2, max_features=0.1, max_depth=100, learning_rate=0.1, total=   0.1s\n",
      "[CV] n_estimators=3, min_samples_split=0.1, min_samples_leaf=0.5, max_leaf_nodes=50, max_features=0.1, max_depth=100, learning_rate=0.5 \n",
      "[CV]  n_estimators=3, min_samples_split=0.1, min_samples_leaf=0.5, max_leaf_nodes=50, max_features=0.1, max_depth=100, learning_rate=0.5, total=   0.0s\n",
      "[CV] n_estimators=3, min_samples_split=0.1, min_samples_leaf=0.5, max_leaf_nodes=50, max_features=0.1, max_depth=100, learning_rate=0.5 \n",
      "[CV]  n_estimators=3, min_samples_split=0.1, min_samples_leaf=0.5, max_leaf_nodes=50, max_features=0.1, max_depth=100, learning_rate=0.5, total=   0.0s\n",
      "[CV] n_estimators=3, min_samples_split=0.1, min_samples_leaf=0.5, max_leaf_nodes=50, max_features=0.1, max_depth=100, learning_rate=0.5 \n",
      "[CV]  n_estimators=3, min_samples_split=0.1, min_samples_leaf=0.5, max_leaf_nodes=50, max_features=0.1, max_depth=100, learning_rate=0.5, total=   0.0s\n",
      "[CV] n_estimators=3, min_samples_split=0.1, min_samples_leaf=0.5, max_leaf_nodes=50, max_features=0.1, max_depth=100, learning_rate=0.5 \n",
      "[CV]  n_estimators=3, min_samples_split=0.1, min_samples_leaf=0.5, max_leaf_nodes=50, max_features=0.1, max_depth=100, learning_rate=0.5, total=   0.0s\n",
      "[CV] n_estimators=3, min_samples_split=0.1, min_samples_leaf=0.1, max_leaf_nodes=50, max_features=0.5, max_depth=100, learning_rate=0.1 \n",
      "[CV]  n_estimators=3, min_samples_split=0.1, min_samples_leaf=0.1, max_leaf_nodes=50, max_features=0.5, max_depth=100, learning_rate=0.1, total=   0.0s\n",
      "[CV] n_estimators=3, min_samples_split=0.1, min_samples_leaf=0.1, max_leaf_nodes=50, max_features=0.5, max_depth=100, learning_rate=0.1 \n",
      "[CV]  n_estimators=3, min_samples_split=0.1, min_samples_leaf=0.1, max_leaf_nodes=50, max_features=0.5, max_depth=100, learning_rate=0.1, total=   0.0s\n",
      "[CV] n_estimators=3, min_samples_split=0.1, min_samples_leaf=0.1, max_leaf_nodes=50, max_features=0.5, max_depth=100, learning_rate=0.1 \n",
      "[CV]  n_estimators=3, min_samples_split=0.1, min_samples_leaf=0.1, max_leaf_nodes=50, max_features=0.5, max_depth=100, learning_rate=0.1, total=   0.0s\n",
      "[CV] n_estimators=3, min_samples_split=0.1, min_samples_leaf=0.1, max_leaf_nodes=50, max_features=0.5, max_depth=100, learning_rate=0.1 \n",
      "[CV]  n_estimators=3, min_samples_split=0.1, min_samples_leaf=0.1, max_leaf_nodes=50, max_features=0.5, max_depth=100, learning_rate=0.1, total=   0.0s\n",
      "[CV]  n_estimators=200, min_samples_split=0.5, min_samples_leaf=0.1, max_leaf_nodes=100, max_features=0.5, max_depth=50, learning_rate=0.1, total=   0.1s\n",
      "[CV] n_estimators=3, min_samples_split=0.1, min_samples_leaf=0.5, max_leaf_nodes=100, max_features=0.5, max_depth=100, learning_rate=0.1 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  n_estimators=3, min_samples_split=0.1, min_samples_leaf=0.5, max_leaf_nodes=100, max_features=0.5, max_depth=100, learning_rate=0.1, total=   0.0s\n",
      "[CV] n_estimators=3, min_samples_split=0.1, min_samples_leaf=0.5, max_leaf_nodes=100, max_features=0.5, max_depth=100, learning_rate=0.1 \n",
      "[CV]  n_estimators=3, min_samples_split=0.1, min_samples_leaf=0.5, max_leaf_nodes=100, max_features=0.5, max_depth=100, learning_rate=0.1, total=   0.0s\n",
      "[CV] n_estimators=3, min_samples_split=0.1, min_samples_leaf=0.5, max_leaf_nodes=100, max_features=0.5, max_depth=100, learning_rate=0.1 \n",
      "[CV]  n_estimators=3, min_samples_split=0.1, min_samples_leaf=0.5, max_leaf_nodes=100, max_features=0.5, max_depth=100, learning_rate=0.1, total=   0.0s\n",
      "[CV] n_estimators=3, min_samples_split=0.1, min_samples_leaf=0.5, max_leaf_nodes=100, max_features=0.5, max_depth=100, learning_rate=0.1 \n",
      "[CV]  n_estimators=3, min_samples_split=0.1, min_samples_leaf=0.5, max_leaf_nodes=100, max_features=0.5, max_depth=100, learning_rate=0.1, total=   0.0s\n",
      "[CV] n_estimators=200, min_samples_split=0.1, min_samples_leaf=0.1, max_leaf_nodes=50, max_features=0.5, max_depth=3, learning_rate=0.5 \n",
      "[CV]  n_estimators=200, min_samples_split=0.1, min_samples_leaf=0.1, max_leaf_nodes=50, max_features=0.5, max_depth=3, learning_rate=0.5, total=   0.1s\n",
      "[CV] n_estimators=200, min_samples_split=0.1, min_samples_leaf=0.1, max_leaf_nodes=50, max_features=0.5, max_depth=3, learning_rate=0.5 \n",
      "[CV]  n_estimators=200, min_samples_split=0.1, min_samples_leaf=0.1, max_leaf_nodes=50, max_features=0.5, max_depth=3, learning_rate=0.5, total=   0.1s\n",
      "[CV] n_estimators=200, min_samples_split=0.1, min_samples_leaf=0.1, max_leaf_nodes=50, max_features=0.5, max_depth=3, learning_rate=0.5 \n",
      "[CV]  n_estimators=200, min_samples_split=0.1, min_samples_leaf=0.1, max_leaf_nodes=50, max_features=0.5, max_depth=3, learning_rate=0.5, total=   0.1s\n",
      "[CV] n_estimators=200, min_samples_split=0.1, min_samples_leaf=0.1, max_leaf_nodes=50, max_features=0.5, max_depth=3, learning_rate=0.5 \n",
      "[CV]  n_estimators=200, min_samples_split=0.1, min_samples_leaf=0.1, max_leaf_nodes=50, max_features=0.5, max_depth=3, learning_rate=0.5, total=   0.1s\n",
      "[CV] n_estimators=3, min_samples_split=0.5, min_samples_leaf=0.1, max_leaf_nodes=2, max_features=0.5, max_depth=50, learning_rate=0.5 \n",
      "[CV]  n_estimators=3, min_samples_split=0.5, min_samples_leaf=0.1, max_leaf_nodes=2, max_features=0.5, max_depth=50, learning_rate=0.5, total=   0.0s\n",
      "[CV] n_estimators=3, min_samples_split=0.5, min_samples_leaf=0.1, max_leaf_nodes=2, max_features=0.5, max_depth=50, learning_rate=0.5 \n",
      "[CV]  n_estimators=3, min_samples_split=0.5, min_samples_leaf=0.1, max_leaf_nodes=2, max_features=0.5, max_depth=50, learning_rate=0.5, total=   0.0s\n",
      "[CV] n_estimators=3, min_samples_split=0.5, min_samples_leaf=0.1, max_leaf_nodes=2, max_features=0.5, max_depth=50, learning_rate=0.5 \n",
      "[CV]  n_estimators=3, min_samples_split=0.5, min_samples_leaf=0.1, max_leaf_nodes=2, max_features=0.5, max_depth=50, learning_rate=0.5, total=   0.0s\n",
      "[CV] n_estimators=3, min_samples_split=0.5, min_samples_leaf=0.1, max_leaf_nodes=2, max_features=0.5, max_depth=50, learning_rate=0.5 \n",
      "[CV]  n_estimators=3, min_samples_split=0.5, min_samples_leaf=0.1, max_leaf_nodes=2, max_features=0.5, max_depth=50, learning_rate=0.5, total=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:    1.2s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=4, error_score='raise',\n",
       "          estimator=GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "              presort='auto', random_state=42, subsample=1.0, verbose=0,\n",
       "              warm_start=False),\n",
       "          fit_params=None, iid=True, n_iter=10, n_jobs=-1,\n",
       "          param_distributions={'n_estimators': [3, 200], 'learning_rate': [0.1, 0.5], 'max_depth': [3, 50, 100], 'min_samples_split': [0.1, 0.5], 'min_samples_leaf': [0.1, 0.5], 'max_features': [0.1, 0.5], 'max_leaf_nodes': [2, 50, 100]},\n",
       "          pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "          return_train_score='warn', scoring='roc_auc', verbose=2)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_boost_class = GradientBoostingClassifier(random_state = 42)\n",
    "\n",
    "n_estimators = [3, 200]\n",
    "learning_rate = [0.1, 0.5]\n",
    "max_depth = [3, 50, 100]\n",
    "min_samples_split = [0.1, 0.5]\n",
    "min_samples_leaf = [0.1, 0.5]\n",
    "max_features = [0.1, 0.5]\n",
    "max_leaf_nodes = [2, 50, 100]\n",
    "                            \n",
    "param_grid_grad_boost_class = {'n_estimators' : n_estimators, 'learning_rate' : learning_rate,\n",
    "                              'max_depth' : max_depth, 'min_samples_split' : min_samples_split,\n",
    "                              'min_samples_leaf' : min_samples_leaf, 'max_features' : max_features,\n",
    "                              'max_leaf_nodes' : max_leaf_nodes}\n",
    "\n",
    "rand_search_grad_boost_class = RandomizedSearchCV(grad_boost_class, param_grid_grad_boost_class, cv = 4, scoring='roc_auc', \n",
    "                               refit = True, n_jobs = -1, verbose = 2)\n",
    "\n",
    "rand_search_grad_boost_class.fit(X_train, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 200, 'min_samples_split': 0.1, 'min_samples_leaf': 0.1, 'max_leaf_nodes': 50, 'max_features': 0.5, 'max_depth': 3, 'learning_rate': 0.5}\n",
      "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
      "              learning_rate=0.5, loss='deviance', max_depth=3,\n",
      "              max_features=0.5, max_leaf_nodes=50,\n",
      "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "              min_samples_leaf=0.1, min_samples_split=0.1,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=200,\n",
      "              presort='auto', random_state=42, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "print(rand_search_grad_boost_class.best_params_)\n",
    "\n",
    "print(rand_search_grad_boost_class.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7867564534231201"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_boost_randomized = rand_search_grad_boost_class.best_estimator_\n",
    "\n",
    "y_pred_grad_boost_randomized = grad_boost_randomized.predict(X_train)\n",
    "accuracy_score(train_labels, y_pred_extra_trees_randomized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[468  81]\n",
      " [109 233]]\n",
      "Precision:  0.7420382165605095\n",
      "Recall:  0.6812865497076024\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(train_labels, y_pred_extra_trees_randomized))\n",
    "\n",
    "print(\"Precision: \", precision_score(train_labels, y_pred_extra_trees_randomized))\n",
    "\n",
    "print(\"Recall: \", recall_score(train_labels, y_pred_extra_trees_randomized))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 10 candidates, totalling 40 fits\n",
      "[CV] C=0.1 ...........................................................\n",
      "[CV] C=0.1 ...........................................................\n",
      "[CV] C=0.1 ...........................................................\n",
      "[CV] C=0.1 ...........................................................\n",
      "[CV] ............................................ C=0.1, total=   0.3s\n",
      "[CV] ............................................ C=0.1, total=   0.3s\n",
      "[CV] C=0.2 ...........................................................\n",
      "[CV] ............................................ C=0.1, total=   0.3s\n",
      "[CV] ............................................ C=0.2, total=   0.0s\n",
      "[CV] C=0.2 ...........................................................\n",
      "[CV] ............................................ C=0.1, total=   0.3s\n",
      "[CV] ............................................ C=0.2, total=   0.0s\n",
      "[CV] C=0.2 ...........................................................\n",
      "[CV] C=0.3 ...........................................................\n",
      "[CV] C=0.2 ...........................................................\n",
      "[CV] ............................................ C=0.3, total=   0.0s\n",
      "[CV] ............................................ C=0.2, total=   0.0s\n",
      "[CV] ............................................ C=0.2, total=   0.0s\n",
      "[CV] C=0.3 ...........................................................\n",
      "[CV] C=0.3 ...........................................................\n",
      "[CV] C=0.4 ...........................................................\n",
      "[CV] C=0.3 ...........................................................\n",
      "[CV] ............................................ C=0.4, total=   0.0s\n",
      "[CV] ............................................ C=0.3, total=   0.0s\n",
      "[CV] ............................................ C=0.3, total=   0.0s\n",
      "[CV] C=0.4 ...........................................................\n",
      "[CV] C=0.4 ...........................................................\n",
      "[CV] ............................................ C=0.3, total=   0.0s\n",
      "[CV] C=0.4 ...........................................................\n",
      "[CV] ............................................ C=0.4, total=   0.0s\n",
      "[CV] C=0.5 ...........................................................\n",
      "[CV] ............................................ C=0.5, total=   0.0s\n",
      "[CV] ............................................ C=0.4, total=   0.0s\n",
      "[CV] C=0.5 ...........................................................\n",
      "[CV] ............................................ C=0.4, total=   0.0s\n",
      "[CV] ............................................ C=0.5, total=   0.0s\n",
      "[CV] C=0.5 ...........................................................\n",
      "[CV] C=0.6 ...........................................................\n",
      "[CV] C=0.5 ...........................................................\n",
      "[CV] ............................................ C=0.6, total=   0.0s\n",
      "[CV] C=0.6 ...........................................................\n",
      "[CV] ............................................ C=0.5, total=   0.0s\n",
      "[CV] C=0.6 ...........................................................\n",
      "[CV] C=0.6 ...........................................................\n",
      "[CV] ............................................ C=0.6, total=   0.0s\n",
      "[CV] ............................................ C=0.6, total=   0.0s\n",
      "[CV] ............................................ C=0.5, total=   0.0s\n",
      "[CV] C=0.7 ...........................................................\n",
      "[CV] ............................................ C=0.6, total=   0.0s\n",
      "[CV] C=0.7 ...........................................................\n",
      "[CV] ............................................ C=0.7, total=   0.0s\n",
      "[CV] ............................................ C=0.7, total=   0.0s\n",
      "[CV] C=0.7 ...........................................................\n",
      "[CV] ............................................ C=0.7, total=   0.0s\n",
      "[CV] C=0.8 ...........................................................\n",
      "[CV] C=0.7 ...........................................................\n",
      "[CV] C=0.9 ...........................................................\n",
      "[CV] C=0.8 ...........................................................\n",
      "[CV] ............................................ C=0.9, total=   0.0s\n",
      "[CV] C=0.9 ...........................................................\n",
      "[CV] ............................................ C=0.8, total=   0.0s\n",
      "[CV] ............................................ C=0.7, total=   0.0s\n",
      "[CV] ............................................ C=0.8, total=   0.0s\n",
      "[CV] C=0.9 ...........................................................\n",
      "[CV] C=0.8 ...........................................................\n",
      "[CV] C=0.8 ...........................................................\n",
      "[CV] ............................................ C=0.8, total=   0.0s\n",
      "[CV] ............................................ C=0.9, total=   0.0s\n",
      "[CV] ............................................ C=0.9, total=   0.0s\n",
      "[CV] ............................................ C=0.8, total=   0.0s\n",
      "[CV] C=0.9 ...........................................................\n",
      "[CV] ............................................ C=0.9, total=   0.0s\n",
      "[CV] C=1.0 ...........................................................\n",
      "[CV] ............................................ C=1.0, total=   0.0s\n",
      "[CV] C=1.0 ...........................................................\n",
      "[CV] ............................................ C=1.0, total=   0.0s\n",
      "[CV] C=1.0 ...........................................................\n",
      "[CV] ............................................ C=1.0, total=   0.0s\n",
      "[CV] C=1.0 ...........................................................\n",
      "[CV] ............................................ C=1.0, total=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  33 out of  40 | elapsed:    0.5s remaining:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:    0.5s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=4, error_score='raise',\n",
       "          estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=42, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "          fit_params=None, iid=True, n_iter=10, n_jobs=-1,\n",
       "          param_distributions={'C': array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])},\n",
       "          pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "          return_train_score='warn', scoring='roc_auc', verbose=2)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_reg = LogisticRegression(random_state = 42)\n",
    "\n",
    "C = np.array(list(range(1, 11)))/10\n",
    "                            \n",
    "param_grid_log_reg = {'C' : C}\n",
    "\n",
    "rand_search_log_reg = RandomizedSearchCV(log_reg, param_grid_log_reg, cv = 4, scoring='roc_auc', \n",
    "                               refit = True, n_jobs = -1, verbose = 2)\n",
    "\n",
    "rand_search_log_reg.fit(X_train, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 1.0}\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=42, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "print(rand_search_log_reg.best_params_)\n",
    "\n",
    "print(rand_search_log_reg.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8002244668911336"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_reg_randomized = rand_search_log_reg.best_estimator_\n",
    "\n",
    "y_pred_log_reg_randomized = log_reg_randomized.predict(X_train)\n",
    "accuracy_score(train_labels, y_pred_log_reg_randomized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[473  76]\n",
      " [102 240]]\n",
      "Precision:  0.759493670886076\n",
      "Recall:  0.7017543859649122\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(train_labels, y_pred_log_reg_randomized))\n",
    "\n",
    "print(\"Precision: \", precision_score(train_labels, y_pred_log_reg_randomized))\n",
    "\n",
    "print(\"Recall: \", recall_score(train_labels, y_pred_log_reg_randomized))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model of Choice\n",
    "\n",
    "As we can see, SVC is clearly a winner since it has the highest accuracy score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "svc_randomized = rnd_search_cv_svc.best_estimator_\n",
    "\n",
    "y_pred_test_svc_randomized = svc_randomized.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's append the necessary info into the dataframe\n",
    "\n",
    "result_test = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result_test[\"PassengerId\"] = passenger_id_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result_test[\"Survived\"] = y_pred_test_svc_randomized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>892</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>893</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>894</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>895</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>896</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>897</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>898</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>899</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>900</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>901</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>902</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>903</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>904</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>905</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>906</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>907</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>908</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>909</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>910</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>911</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>912</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>913</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>914</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>915</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>916</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>917</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>918</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>919</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>920</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>921</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388</th>\n",
       "      <td>1280</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389</th>\n",
       "      <td>1281</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390</th>\n",
       "      <td>1282</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>391</th>\n",
       "      <td>1283</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392</th>\n",
       "      <td>1284</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393</th>\n",
       "      <td>1285</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394</th>\n",
       "      <td>1286</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>1287</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>1288</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>1289</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>1290</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>1291</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>1292</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401</th>\n",
       "      <td>1293</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>402</th>\n",
       "      <td>1294</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403</th>\n",
       "      <td>1295</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404</th>\n",
       "      <td>1296</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>405</th>\n",
       "      <td>1297</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406</th>\n",
       "      <td>1298</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>407</th>\n",
       "      <td>1299</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>408</th>\n",
       "      <td>1300</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>409</th>\n",
       "      <td>1301</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>410</th>\n",
       "      <td>1302</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>1303</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412</th>\n",
       "      <td>1304</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>1305</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>1306</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>1307</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>1308</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>1309</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>418 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     PassengerId  Survived\n",
       "0            892         0\n",
       "1            893         0\n",
       "2            894         0\n",
       "3            895         0\n",
       "4            896         0\n",
       "5            897         0\n",
       "6            898         1\n",
       "7            899         0\n",
       "8            900         1\n",
       "9            901         0\n",
       "10           902         0\n",
       "11           903         0\n",
       "12           904         1\n",
       "13           905         0\n",
       "14           906         1\n",
       "15           907         1\n",
       "16           908         0\n",
       "17           909         0\n",
       "18           910         0\n",
       "19           911         1\n",
       "20           912         0\n",
       "21           913         0\n",
       "22           914         1\n",
       "23           915         0\n",
       "24           916         1\n",
       "25           917         0\n",
       "26           918         1\n",
       "27           919         0\n",
       "28           920         0\n",
       "29           921         0\n",
       "..           ...       ...\n",
       "388         1280         0\n",
       "389         1281         0\n",
       "390         1282         0\n",
       "391         1283         1\n",
       "392         1284         1\n",
       "393         1285         0\n",
       "394         1286         0\n",
       "395         1287         1\n",
       "396         1288         0\n",
       "397         1289         1\n",
       "398         1290         0\n",
       "399         1291         0\n",
       "400         1292         1\n",
       "401         1293         0\n",
       "402         1294         1\n",
       "403         1295         0\n",
       "404         1296         0\n",
       "405         1297         0\n",
       "406         1298         0\n",
       "407         1299         0\n",
       "408         1300         1\n",
       "409         1301         0\n",
       "410         1302         1\n",
       "411         1303         1\n",
       "412         1304         0\n",
       "413         1305         0\n",
       "414         1306         1\n",
       "415         1307         0\n",
       "416         1308         0\n",
       "417         1309         0\n",
       "\n",
       "[418 rows x 2 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result_test.to_csv(\"result_test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a Validation Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Take 20% of the train set as the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "891\n",
      "12\n",
      "178\n",
      "713\n"
     ]
    }
   ],
   "source": [
    "rows, cols = X_train.shape\n",
    "\n",
    "print(rows)\n",
    "print(cols)\n",
    "\n",
    "\n",
    "boundary = int(0.2*rows)\n",
    "print(boundary)\n",
    "print(rows - boundary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = X_train[(891-boundary):892,]\n",
    "y_val = train_labels[(891-boundary):892:,]\n",
    "\n",
    "X_train = X_train[:713,]\n",
    "train_labels = train_labels[:713,]\n",
    "\n",
    "# X_train, train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    434\n",
       "1    279\n",
       "Name: Survived, dtype: int64"
      ]
     },
     "execution_count": 365,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    115\n",
       "1     63\n",
       "Name: Survived, dtype: int64"
      ]
     },
     "execution_count": 366,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(713,)"
      ]
     },
     "execution_count": 367,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "178\n"
     ]
    }
   ],
   "source": [
    "print(X_val.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "713\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "891\n"
     ]
    }
   ],
   "source": [
    "print(X_val.shape[0] + X_train.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_inputs = 12\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 200\n",
    "n_hidden3 = 100\n",
    "n_outputs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.float32, shape=(None), name=\"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement dropout\n",
    "\n",
    "training = tf.placeholder_with_default(False, shape=(), name='training')\n",
    "\n",
    "dropout_rate = 0.5  # == 1 - keep_prob\n",
    "X_drop = tf.layers.dropout(X, dropout_rate, training=training)\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X_drop, n_hidden1, name=\"hidden1\",\n",
    "                              activation=tf.nn.selu)\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, name=\"hidden2\",\n",
    "                              activation=tf.nn.selu)\n",
    "    hidden3 = tf.layers.dense(hidden2, n_hidden3, name=\"hidden3\",\n",
    "                              activation=tf.nn.selu)\n",
    "    logit = tf.layers.dense(hidden3, n_outputs, name=\"outputs\")\n",
    "    logits = logit[:, 0] # tf.transpose is new\n",
    "    #logits = np.reshape(logits, (-1, 1)) # NEW!\n",
    "    logits = tf.reshape(logits, [-1, 1]) # NEW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"loss\"):\n",
    "    #xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits) # tf.transpose is new\n",
    "    xentropy = tf.nn.sigmoid_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "    loss_summary = tf.summary.scalar('log_loss', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, tf.cast(y, tf.int64), 1) # tf.cast is new\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    accuracy_summary = tf.summary.scalar('accuracy', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def log_dir(prefix=\"\"):\n",
    "    now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "    root_logdir = \"tf_logs\"\n",
    "    if prefix:\n",
    "        prefix += \"-\"\n",
    "    name = prefix + \"run-\" + now\n",
    "    return \"{}/{}/\".format(root_logdir, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logdir = log_dir(\"Titanic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m, n = X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def random_batch(X_train, y_train, batch_size):\n",
    "    rnd_indices = np.random.randint(0, len(X_train), batch_size)\n",
    "#    rnd_indices = np.reshape(rnd_indices, (-1, 1))\n",
    "    X_batch = X_train[rnd_indices]\n",
    "    y_batch = y_train[rnd_indices] # OLD! DO NOT DELETE!\n",
    "    #y_batch = np.array(y_train)\n",
    "    #y_batch = y_batch[rnd_indices]\n",
    "    return X_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "targets[3] is out of range\n\t [[Node: eval/in_top_k/InTopKV2 = InTopKV2[T=DT_INT64, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](dnn/Reshape, eval/Cast, eval/in_top_k/InTopKV2/k)]]\n\nCaused by op 'eval/in_top_k/InTopKV2', defined at:\n  File \"/home/isaac/anaconda3/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/isaac/anaconda3/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/isaac/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/isaac/anaconda3/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/isaac/anaconda3/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/home/isaac/anaconda3/lib/python3.6/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/isaac/anaconda3/lib/python3.6/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/home/isaac/anaconda3/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/isaac/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/home/isaac/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/isaac/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/isaac/anaconda3/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/isaac/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/isaac/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/isaac/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/isaac/anaconda3/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/isaac/anaconda3/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/isaac/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/isaac/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2850, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/isaac/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-377-e695d2645e60>\", line 2, in <module>\n    correct = tf.nn.in_top_k(logits, tf.cast(y, tf.int64), 1) # tf.cast is new\n  File \"/home/isaac/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\", line 2311, in in_top_k\n    return gen_nn_ops._in_top_kv2(predictions, targets, k, name=name)\n  File \"/home/isaac/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 2550, in _in_top_kv2\n    \"InTopKV2\", predictions=predictions, targets=targets, k=k, name=name)\n  File \"/home/isaac/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/isaac/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2956, in create_op\n    op_def=op_def)\n  File \"/home/isaac/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1470, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): targets[3] is out of range\n\t [[Node: eval/in_top_k/InTopKV2 = InTopKV2[T=DT_INT64, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](dnn/Reshape, eval/Cast, eval/in_top_k/InTopKV2/k)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    472\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    474\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: targets[3] is out of range\n\t [[Node: eval/in_top_k/InTopKV2 = InTopKV2[T=DT_INT64, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](dnn/Reshape, eval/Cast, eval/in_top_k/InTopKV2/k)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-384-ed7fcc2943ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;31m#X_batch, y_batch = X_train(batch_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0maccuracy_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy_summary_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_summary_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy_summary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_summary\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0mfile_writer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy_summary_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mfile_writer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_summary_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1334\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1335\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1336\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1338\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: targets[3] is out of range\n\t [[Node: eval/in_top_k/InTopKV2 = InTopKV2[T=DT_INT64, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](dnn/Reshape, eval/Cast, eval/in_top_k/InTopKV2/k)]]\n\nCaused by op 'eval/in_top_k/InTopKV2', defined at:\n  File \"/home/isaac/anaconda3/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/isaac/anaconda3/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/isaac/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/isaac/anaconda3/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/isaac/anaconda3/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/home/isaac/anaconda3/lib/python3.6/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/isaac/anaconda3/lib/python3.6/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/home/isaac/anaconda3/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/isaac/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/home/isaac/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/isaac/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/isaac/anaconda3/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/isaac/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/isaac/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/isaac/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/isaac/anaconda3/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/isaac/anaconda3/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/isaac/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/isaac/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2850, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/isaac/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-377-e695d2645e60>\", line 2, in <module>\n    correct = tf.nn.in_top_k(logits, tf.cast(y, tf.int64), 1) # tf.cast is new\n  File \"/home/isaac/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\", line 2311, in in_top_k\n    return gen_nn_ops._in_top_kv2(predictions, targets, k, name=name)\n  File \"/home/isaac/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 2550, in _in_top_kv2\n    \"InTopKV2\", predictions=predictions, targets=targets, k=k, name=name)\n  File \"/home/isaac/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/isaac/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2956, in create_op\n    op_def=op_def)\n  File \"/home/isaac/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1470, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): targets[3] is out of range\n\t [[Node: eval/in_top_k/InTopKV2 = InTopKV2[T=DT_INT64, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](dnn/Reshape, eval/Cast, eval/in_top_k/InTopKV2/k)]]\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 51\n",
    "batch_size = 5\n",
    "n_batches = int(np.ceil(m / batch_size))\n",
    "\n",
    "checkpoint_path = \"/tmp/titanic.ckpt\"\n",
    "checkpoint_epoch_path = checkpoint_path + \".epoch\"\n",
    "final_model = \"./titanic\"\n",
    "\n",
    "best_loss = np.infty\n",
    "epochs_without_progress = 0\n",
    "max_epochs_without_progress = 50\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    if os.path.isfile(checkpoint_epoch_path):\n",
    "        # if the checkpoint file exists, restore the model and load the epoch number\n",
    "        with open(checkpoint_epoch_path, \"rb\") as f:\n",
    "            start_epoch = int(f.read())\n",
    "        print(\"Training was interrupted. Continuing at epoch\", start_epoch)\n",
    "        saver.restore(sess, checkpoint_path)\n",
    "    else:\n",
    "        start_epoch = 0\n",
    "        sess.run(init)\n",
    "\n",
    "    for epoch in range(start_epoch, n_epochs):\n",
    "        for iteration in range(n_batches):\n",
    "            X_batch, y_batch = random_batch(X_train, np.array(train_labels), batch_size)\n",
    "            #X_batch, y_batch = X_train(batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val, loss_val, accuracy_summary_str, loss_summary_str = sess.run([accuracy, loss, accuracy_summary, loss_summary], feed_dict={X: X_val, y: y_val})\n",
    "        file_writer.add_summary(accuracy_summary_str, epoch)\n",
    "        file_writer.add_summary(loss_summary_str, epoch)\n",
    "        if epoch % 5 == 0:\n",
    "            print(\"Epoch:\", epoch,\n",
    "                  \"\\tValidation accuracy: {:.3f}%\".format(accuracy_val * 100),\n",
    "                  \"\\tLoss: {:.5f}\".format(loss_val))\n",
    "            saver.save(sess, checkpoint_path)\n",
    "            with open(checkpoint_epoch_path, \"wb\") as f:\n",
    "                f.write(b\"%d\" % (epoch + 1))\n",
    "            if loss_val < best_loss:\n",
    "                saver.save(sess, final_model_path)\n",
    "                best_loss = loss_val\n",
    "            else:\n",
    "                epochs_without_progress += 5\n",
    "                if epochs_without_progress > max_epochs_without_progress:\n",
    "                    print(\"Early stopping\")\n",
    "                    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for array",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-333-b062216931e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m339\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m330\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m639\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m505\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m347\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m472\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m230\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m189\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m384\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for array"
     ]
    }
   ],
   "source": [
    "X_train[339, 330, 639, 505, 347, 472, 230, 189, 224,384]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 343,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
